{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from time import time"
      ],
      "metadata": {
        "id": "SSUk52pWO_3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_split_data(path='./', validation_fraction=0.1, delimiter='::', random_seed=1234, should_transpose=False):\n",
        "    '''\n",
        "    Loads the MovieLens 1M dataset, splits it into training and validation sets, and returns them as dense matrices.\n",
        "\n",
        "    :param path: path to the ratings file\n",
        "    :param validation_fraction: fraction of data to use for validation\n",
        "    :param delimiter: delimiter used in the data file\n",
        "    :param random_seed: random seed for validation splitting\n",
        "    :param should_transpose: flag to transpose output matrices (swapping users with movies)\n",
        "    :return: train ratings (n_u, n_m), valid ratings (n_u, n_m)\n",
        "    '''\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "    start_time = time()\n",
        "    print('Reading data...')\n",
        "    data = np.genfromtxt(path, delimiter=delimiter).astype('int32')\n",
        "    print('Data read in', time() - start_time, 'seconds')\n",
        "\n",
        "    num_users = np.unique(data[:, 0]).shape[0]\n",
        "    num_movies = np.unique(data[:, 1]).shape[0]\n",
        "    num_ratings = data.shape[0]\n",
        "\n",
        "    # Create dictionaries that map user/movie IDs to contiguous user/movie numbers\n",
        "    user_map = {user_id: index for index, user_id in enumerate(np.unique(data[:, 0]))}\n",
        "    movie_map = {movie_id: index for index, movie_id in enumerate(np.unique(data[:, 1]))}\n",
        "\n",
        "    # Shuffle indices\n",
        "    indices = np.arange(num_ratings)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    train_ratings = np.zeros((num_users, num_movies), dtype='float32')\n",
        "    valid_ratings = np.zeros((num_users, num_movies), dtype='float32')\n",
        "\n",
        "    # Iterate through shuffled indices and assign ratings to training and validation sets\n",
        "    for i in range(num_ratings):\n",
        "        user_id = data[indices[i], 0]\n",
        "        movie_id = data[indices[i], 1]\n",
        "        rating = data[indices[i], 2]\n",
        "\n",
        "        # The first few ratings of the shuffled data array are validation data\n",
        "        if i <= validation_fraction * num_ratings:\n",
        "            valid_ratings[user_map[user_id], movie_map[movie_id]] = int(rating)\n",
        "        # The rest are training data\n",
        "        else:\n",
        "            train_ratings[user_map[user_id], movie_map[movie_id]] = int(rating)\n",
        "\n",
        "    # Transpose the matrices if the transpose flag is set\n",
        "    if should_transpose:\n",
        "        train_ratings = train_ratings.T\n",
        "        valid_ratings = valid_ratings.T\n",
        "\n",
        "    print('Loaded dense data matrix')\n",
        "\n",
        "    return train_ratings, valid_ratings\n"
      ],
      "metadata": {
        "id": "wakd-SLGOcid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78zrZJZW2Wiy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import sys\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = int(time())\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "metadata": {
        "id": "uEB_ZBzf3C3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e3f9ad5-5503-452e-f411-14383929f751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f3fab1a4110>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "tr, vr = load_and_split_data('ratings.dat', validation_fraction=0.1, delimiter='::',random_seed=seed, should_transpose=True)\n",
        "\n",
        "tm = np.greater(tr, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
        "vm = np.greater(vr, 1e-12).astype('float32')\n",
        "\n",
        "n_m = tr.shape[0]  # number of movies\n",
        "n_u = tr.shape[1]  # number of users (may be switched depending on 'transpose' in loadData)\n"
      ],
      "metadata": {
        "id": "PlX_l32B3Fip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7ea83ca-cae5-42bb-9e5f-939ff9f85adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading data...\n",
            "Data read in 3.1598000526428223 seconds\n",
            "Loaded dense data matrix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyper-parameters\n",
        "n_hid = 700\n",
        "lambda_2 = 60.0\n",
        "lambda_s = 0.013\n",
        "n_layers = 3\n",
        "output_every = 50  #evaluate performance on test set; breaks l-bfgs loop\n",
        "n_epoch = n_layers * 10 * output_every\n",
        "batch_size = 256  # new parameter to control the batch size\n",
        "\n",
        "verbose_bfgs = True\n",
        "use_gpu = True\n",
        "\n",
        "if use_gpu and torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "tr = torch.tensor(tr, dtype=torch.float32, device=device)\n",
        "tm = torch.tensor(tm, dtype=torch.float32, device=device)\n",
        "vr = torch.tensor(vr, dtype=torch.float32, device=device)\n",
        "vm = torch.tensor(vm, dtype=torch.float32, device=device)"
      ],
      "metadata": {
        "id": "J4PyoHAMQcL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KernelLayer(nn.Module):\n",
        "    def __init__(self, n_in, n_hid=500, n_dim=5, activation=nn.Sigmoid(),\n",
        "                 lambda_s=lambda_s, lambda_2=lambda_2):\n",
        "        super(KernelLayer, self).__init__()\n",
        "\n",
        "        self.W = nn.Parameter(torch.empty(n_in, n_hid).normal_(std=1e-3))\n",
        "        self.u = nn.Parameter(torch.empty(n_in, 1, n_dim).normal_(std=1e-3))\n",
        "        self.v = nn.Parameter(torch.empty(1, n_hid, n_dim).normal_(std=1e-3))\n",
        "        self.b = nn.Parameter(torch.zeros(n_hid))\n",
        "        self.activation = activation\n",
        "        self.lambda_s = lambda_s\n",
        "        self.lambda_2 = lambda_2\n",
        "\n",
        "    def kernel(self, u, v):\n",
        "        dist = torch.norm(u - v, p=2, dim=2)\n",
        "        hat = torch.clamp(1.0 - dist ** 2, 0.0)\n",
        "        return hat\n",
        "\n",
        "    def forward(self, x):\n",
        "        w_hat = self.kernel(self.u, self.v)\n",
        "        W_eff = self.W * w_hat\n",
        "        y = x @ W_eff + self.b\n",
        "        y = self.activation(y)\n",
        "        return y\n",
        "\n",
        "    def regularization(self):\n",
        "        w_hat = self.kernel(self.u, self.v)\n",
        "        sparse_reg_term = self.lambda_s * torch.norm(w_hat, p=2)\n",
        "        l2_reg_term = self.lambda_2 * torch.norm(self.W, p=2)\n",
        "        return sparse_reg_term + l2_reg_term"
      ],
      "metadata": {
        "id": "yL6IipQg3Oau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KernelNet(nn.Module):\n",
        "    def __init__(self, n_u, n_hid, n_layers, lambda_s, lambda_2):\n",
        "        super(KernelNet, self).__init__()\n",
        "\n",
        "        # Create the hidden layers and output layer using KernelLayer class\n",
        "        layers = [KernelLayer(n_u if i == 0 else n_hid, n_hid, lambda_s=lambda_s, lambda_2=lambda_2)\n",
        "                  for i in range(n_layers)]\n",
        "        layers.append(KernelLayer(n_hid, n_u, activation=nn.Identity(), lambda_s=lambda_s, lambda_2=lambda_2))\n",
        "\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "            # Pass the input through all the layers in the network\n",
        "            for layer in self.layers:\n",
        "                x = layer(x)\n",
        "            return x\n",
        "\n",
        "    def regularization(self):\n",
        "        # Compute the regularization loss for the entire network\n",
        "        reg_losses = sum(layer.regularization() for layer in self.layers)\n",
        "        return reg_losses\n"
      ],
      "metadata": {
        "id": "O3YomPpd3R01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, tr, tm, vr, vm, n_epoch, output_every, device):\n",
        "    tr = tr.float().to(device)\n",
        "    tm = tm.float().to(device)\n",
        "    vr = vr.float().to(device)\n",
        "    vm = vm.float().to(device)\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.LBFGS(model.parameters(), lr=0.01, max_iter=output_every)\n",
        "\n",
        "    def closure():\n",
        "        optimizer.zero_grad()\n",
        "        prediction = model(tr)\n",
        "        diff = tm * (tr - prediction)\n",
        "        sqE = torch.norm(diff, p=2)\n",
        "        loss = sqE + model.regularization()\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    for i in range(int(n_epoch / output_every)):\n",
        "        optimizer.step(closure)\n",
        "        with torch.no_grad():\n",
        "            prediction = model(tr)\n",
        "            error = ((vm * (torch.clamp(prediction, 1., 5.) - vr) ** 2).sum() / vm.sum()).sqrt().item()\n",
        "            error_train = ((tm * (torch.clamp(prediction, 1., 5.) - tr) ** 2).sum() / tm.sum()).sqrt().item()\n",
        "\n",
        "            print('.-^-._' * 12)\n",
        "            print('epoch:', i, 'validation rmse:', error, 'train rmse:', error_train)\n",
        "            print('.-^-._' * 12)\n",
        "\n",
        "    with open('summary_ml1m.txt', 'a') as file:\n",
        "        for a in sys.argv[1:]:\n",
        "            file.write(a + ' ')\n",
        "        file.write(str(error) + ' ' + str(error_train) + ' ' + str(seed) + '\\n')\n",
        "        file.close()"
      ],
      "metadata": {
        "id": "IpyWtLQt3UNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = KernelNet(n_u, n_hid, n_layers, lambda_s, lambda_2)\n",
        "train(model, tr, tm, vr, vm, n_epoch, output_every, device)"
      ],
      "metadata": {
        "id": "qH5bDwSlRidH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a67f2ecc-2161-4312-e36d-77c6f3121edc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 0 validation rmse: 1.8169916868209839 train rmse: 1.811744213104248\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 1 validation rmse: 1.3099744319915771 train rmse: 1.2994760274887085\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 2 validation rmse: 1.1581028699874878 train rmse: 1.1482983827590942\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 3 validation rmse: 1.2342067956924438 train rmse: 1.2272725105285645\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 4 validation rmse: 1.1273736953735352 train rmse: 1.1184077262878418\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 5 validation rmse: 1.116466999053955 train rmse: 1.1077183485031128\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 6 validation rmse: 1.109440565109253 train rmse: 1.1007565259933472\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 7 validation rmse: 1.1052757501602173 train rmse: 1.096611499786377\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 8 validation rmse: 1.1031692028045654 train rmse: 1.0945073366165161\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 9 validation rmse: 1.1020628213882446 train rmse: 1.0933983325958252\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 10 validation rmse: 1.101375699043274 train rmse: 1.0927069187164307\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 11 validation rmse: 1.1009596586227417 train rmse: 1.0922865867614746\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 12 validation rmse: 1.0995415449142456 train rmse: 1.0908308029174805\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 13 validation rmse: 1.096432089805603 train rmse: 1.0876390933990479\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 14 validation rmse: 1.0942224264144897 train rmse: 1.085355281829834\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 15 validation rmse: 1.0931529998779297 train rmse: 1.084246039390564\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 16 validation rmse: 1.09184730052948 train rmse: 1.082890272140503\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 17 validation rmse: 1.0911575555801392 train rmse: 1.082173228263855\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 18 validation rmse: 1.0911211967468262 train rmse: 1.0821356773376465\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 19 validation rmse: 1.0910755395889282 train rmse: 1.0820879936218262\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 20 validation rmse: 1.0910640954971313 train rmse: 1.0820761919021606\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 21 validation rmse: 1.0910392999649048 train rmse: 1.0820503234863281\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 22 validation rmse: 1.0910035371780396 train rmse: 1.0820131301879883\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 23 validation rmse: 1.0909934043884277 train rmse: 1.0820025205612183\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 24 validation rmse: 1.0909847021102905 train rmse: 1.081993579864502\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 25 validation rmse: 1.0909802913665771 train rmse: 1.0819889307022095\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 26 validation rmse: 1.0909771919250488 train rmse: 1.0819858312606812\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 27 validation rmse: 1.0909419059753418 train rmse: 1.0819491147994995\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 28 validation rmse: 1.0909359455108643 train rmse: 1.0819429159164429\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "epoch: 29 validation rmse: 1.0909242630004883 train rmse: 1.0819307565689087\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n"
          ]
        }
      ]
    }
  ]
}