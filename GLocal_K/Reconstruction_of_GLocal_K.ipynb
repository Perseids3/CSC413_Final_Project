{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Original Paper: \n",
        "\n",
        "https://arxiv.org/abs/2108.12184\n",
        "\n",
        "Original code and dataset: \n",
        "\n",
        "https://github.com/usydnlp/Glocal_K\n",
        "\n",
        "One part of our project is the reconstruction is \n",
        "the reconstruction of the GLocal_K algorithm with a different approach: we replaced MOST of tensorflow codes with pytorch, which essentially means to redo all the key parts regarding to the model. Other than that, we basically follow the original ideas of the paper and structure of the existing code. \n",
        "\n",
        "There is one exception (a helpful function) that we used tensorflow instead of pytorch for simplicity."
      ],
      "metadata": {
        "id": "c-Sxlup5gZgk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty3gYQgtnwFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8888256a-d605-401b-a925-28b5e453b0a3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from zipfile import ZipFile\n",
        "file_name = '/content/data.zip'\n",
        "\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl2tU6kL8Ot3"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from time import time\n",
        "from scipy.sparse import csc_matrix\n",
        "import numpy as np\n",
        "import h5py"
      ],
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4A9uU1WloQ2"
      },
      "source": [
        "# Data Loader Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq3KEUaVo1o3"
      },
      "source": [
        "def load_data_100k(path='./', delimiter='\\t'):\n",
        "\n",
        "    train = np.loadtxt(path+'movielens_100k_u1.base', skiprows=0, delimiter=delimiter).astype('int32')\n",
        "    test = np.loadtxt(path+'movielens_100k_u1.test', skiprows=0, delimiter=delimiter).astype('int32')\n",
        "    total = np.concatenate((train, test), axis=0)\n",
        "\n",
        "    n_u = np.unique(total[:,0]).size  # num of users\n",
        "    n_m = np.unique(total[:,1]).size  # num of movies\n",
        "    n_train = train.shape[0]  # num of training ratings\n",
        "    n_test = test.shape[0]  # num of test ratings\n",
        "\n",
        "    train_r = np.zeros((n_m, n_u), dtype='float32')\n",
        "    test_r = np.zeros((n_m, n_u), dtype='float32')\n",
        "\n",
        "    for i in range(n_train):\n",
        "        train_r[train[i,1]-1, train[i,0]-1] = train[i,2]\n",
        "\n",
        "    for i in range(n_test):\n",
        "        test_r[test[i,1]-1, test[i,0]-1] = test[i,2]\n",
        "\n",
        "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
        "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
        "\n",
        "    print('data matrix loaded')\n",
        "    print('num of users: {}'.format(n_u))\n",
        "    print('num of movies: {}'.format(n_m))\n",
        "    print('num of training ratings: {}'.format(n_train))\n",
        "    print('num of test ratings: {}'.format(n_test))\n",
        "\n",
        "    return n_m, n_u, train_r, train_m, test_r, test_m"
      ],
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3e8Xg3us8g7"
      },
      "source": [
        "def load_data_1m(path='./', delimiter='::', frac=0.1, seed=1234):\n",
        "\n",
        "    tic = time()\n",
        "    print('reading data...')\n",
        "    data = np.loadtxt(path+'movielens_1m_dataset.dat', skiprows=0, delimiter=delimiter).astype('int32')\n",
        "    print('taken', time() - tic, 'seconds')\n",
        "\n",
        "    n_u = np.unique(data[:,0]).size  # num of users\n",
        "    n_m = np.unique(data[:,1]).size  # num of movies\n",
        "    n_r = data.shape[0]  # num of ratings\n",
        "\n",
        "    udict = {}\n",
        "    for i, u in enumerate(np.unique(data[:,0]).tolist()):\n",
        "        udict[u] = i\n",
        "    mdict = {}\n",
        "    for i, m in enumerate(np.unique(data[:,1]).tolist()):\n",
        "        mdict[m] = i\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    idx = np.arange(n_r)\n",
        "    np.random.shuffle(idx)\n",
        "\n",
        "    train_r = np.zeros((n_m, n_u), dtype='float32')\n",
        "    test_r = np.zeros((n_m, n_u), dtype='float32')\n",
        "\n",
        "    for i in range(n_r):\n",
        "        u_id = data[idx[i], 0]\n",
        "        m_id = data[idx[i], 1]\n",
        "        r = data[idx[i], 2]\n",
        "\n",
        "        if i < int(frac * n_r):\n",
        "            test_r[mdict[m_id], udict[u_id]] = r\n",
        "        else:\n",
        "            train_r[mdict[m_id], udict[u_id]] = r\n",
        "\n",
        "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
        "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
        "\n",
        "    print('data matrix loaded')\n",
        "    print('num of users: {}'.format(n_u))\n",
        "    print('num of movies: {}'.format(n_m))\n",
        "    print('num of training ratings: {}'.format(n_r - int(frac * n_r)))\n",
        "    print('num of test ratings: {}'.format(int(frac * n_r)))\n",
        "\n",
        "    return n_m, n_u, train_r, train_m, test_r, test_m"
      ],
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rMjcbLvhtRs"
      },
      "source": [
        "def load_matlab_file(path_file, name_field):\n",
        "    \n",
        "    db = h5py.File(path_file, 'r')\n",
        "    ds = db[name_field]\n",
        "\n",
        "    try:\n",
        "        if 'ir' in ds.keys():\n",
        "            data = np.asarray(ds['data'])\n",
        "            ir   = np.asarray(ds['ir'])\n",
        "            jc   = np.asarray(ds['jc'])\n",
        "            out  = csc_matrix((data, ir, jc)).astype(np.float32)\n",
        "    except AttributeError:\n",
        "        out = np.asarray(ds).astype(np.float32).T\n",
        "\n",
        "    db.close()\n",
        "\n",
        "    return out"
      ],
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6pIUrkza2zv"
      },
      "source": [
        "def load_data_monti(path='./'):\n",
        "\n",
        "    M = load_matlab_file(path+'douban_monti_dataset.mat', 'M')\n",
        "    Otraining = load_matlab_file(path+'douban_monti_dataset.mat', 'Otraining') * M\n",
        "    Otest = load_matlab_file(path+'douban_monti_dataset.mat', 'Otest') * M\n",
        "\n",
        "    n_u = M.shape[0]  # num of users\n",
        "    n_m = M.shape[1]  # num of movies\n",
        "    n_train = Otraining[np.where(Otraining)].size  # num of training ratings\n",
        "    n_test = Otest[np.where(Otest)].size  # num of test ratings\n",
        "\n",
        "    train_r = Otraining.T\n",
        "    test_r = Otest.T\n",
        "\n",
        "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
        "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
        "\n",
        "    print('data matrix loaded')\n",
        "    print('num of users: {}'.format(n_u))\n",
        "    print('num of movies: {}'.format(n_m))\n",
        "    print('num of training ratings: {}'.format(n_train))\n",
        "    print('num of test ratings: {}'.format(n_test))\n",
        "\n",
        "    return n_m, n_u, train_r, train_m, test_r, test_m"
      ],
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_8kEkg9mlIW"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fkA1WpmipzF"
      },
      "source": [
        "# Insert the path of a data directory by yourself (e.g., '/content/.../data')\n",
        "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
        "data_path = '/content/data'\n",
        "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._"
      ],
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijlu0lXQioYM"
      },
      "source": [
        "# Select a dataset among 'ML-1M', 'ML-100K', and 'Douban'\n",
        "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
        "dataset = 'ML-100K'\n",
        "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._"
      ],
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJqSSY33mgkw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9f6f73e-3671-416c-a91d-5267bc6373ab"
      },
      "source": [
        "# Data Load\n",
        "try:\n",
        "    if dataset == 'ML-100K':\n",
        "        path = data_path + '/MovieLens_100K/'\n",
        "        n_m, n_u, train_r, train_m, test_r, test_m = load_data_100k(path=path, delimiter='\\t')\n",
        "\n",
        "    elif dataset == 'ML-1M':\n",
        "        path = data_path + '/MovieLens_1M/'\n",
        "        n_m, n_u, train_r, train_m, test_r, test_m = load_data_1m(path=path, delimiter='::', frac=0.1, seed=1234)\n",
        "\n",
        "    elif dataset == 'Douban':\n",
        "        path = data_path + '/Douban_monti/'\n",
        "        n_m, n_u, train_r, train_m, test_r, test_m = load_data_monti(path=path)\n",
        "\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "except ValueError:\n",
        "    print('Error: Unable to load data')\n",
        "\n",
        "# Dim of data\n",
        "print(\"train_r: \", train_r.shape)\n",
        "print(\"train_m: \", train_m.shape)\n",
        "print(\"test_r: \", test_r.shape)\n",
        "print(\"test_m: \", test_m.shape)\n",
        "print(\"n_m: \", n_m)\n",
        "print(\"n_u: \", n_u)"
      ],
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data matrix loaded\n",
            "num of users: 943\n",
            "num of movies: 1682\n",
            "num of training ratings: 80000\n",
            "num of test ratings: 20000\n",
            "train_r:  (1682, 943)\n",
            "train_m:  (1682, 943)\n",
            "test_r:  (1682, 943)\n",
            "test_m:  (1682, 943)\n",
            "n_m:  1682\n",
            "n_u:  943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQMtA9yml-gp"
      },
      "source": [
        "# Hyperparameter Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGCdp_FlobOK"
      },
      "source": [
        "# Common hyperparameter settings\n",
        "n_hid = 500\n",
        "n_dim = 5\n",
        "n_layers = 2\n",
        "gk_size = 3"
      ],
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "344bwGB0cWXp"
      },
      "source": [
        "# Different hyperparameter settings for each dataset\n",
        "if dataset == 'ML-100K':\n",
        "    lambda_2 = 20.  # l2 regularisation\n",
        "    lambda_s = 0.006\n",
        "    iter_p = 5  # optimisation\n",
        "    iter_f = 5\n",
        "    epoch_p = 30  # training epoch\n",
        "    epoch_f = 60\n",
        "    dot_scale = 1  # scaled dot product\n",
        "\n",
        "elif dataset == 'ML-1M':\n",
        "    lambda_2 = 70.\n",
        "    lambda_s = 0.018\n",
        "    iter_p = 50\n",
        "    iter_f = 10\n",
        "    epoch_p = 20\n",
        "    epoch_f = 30\n",
        "    dot_scale = 0.5\n",
        "\n",
        "elif dataset == 'Douban':\n",
        "    lambda_2 = 10.\n",
        "    lambda_s = 0.022\n",
        "    iter_p = 5\n",
        "    iter_f = 5\n",
        "    epoch_p = 20\n",
        "    epoch_f = 60\n",
        "    dot_scale = 2"
      ],
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b94aimX3nAMI"
      },
      "source": [
        "input_shape = (n_m, n_u)\n",
        "\n",
        "# Create an input tensor\n",
        "R = torch.zeros(input_shape, dtype=torch.float32, requires_grad=True)\n",
        "# print(R.shape)"
      ],
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sWtU4-pmDDT"
      },
      "source": [
        "# Network Function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Kernel:"
      ],
      "metadata": {
        "id": "voMCI9KwN4tP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def local_kernel(u, v):\n",
        "    dist = torch.norm(u - v, dim=2, p=2)\n",
        "    hat = torch.clamp(1 - dist**2, min=0)\n",
        "    return hat"
      ],
      "metadata": {
        "id": "0x-JEzl7N8bn"
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Global Kernel"
      ],
      "metadata": {
        "id": "eFEGu8f4sShb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def global_kernel(input, gk_size, dot_scale):\n",
        "    avg_pooling = torch.mean(input, dim=1).unsqueeze(0)\n",
        "    n_kernel = avg_pooling.size(1)\n",
        "\n",
        "    conv_kernel = nn.Parameter(torch.empty(n_kernel, gk_size**2).normal_(0, 0.1))\n",
        "    gk = torch.matmul(avg_pooling, conv_kernel) * dot_scale\n",
        "    gk = gk.view(gk_size, gk_size, 1, 1)\n",
        "\n",
        "    return gk\n",
        "\n",
        "def global_conv(input, W):\n",
        "\n",
        "    input = tf.reshape(input, [1, input.shape[0], input.shape[1], 1])\n",
        "    # print(\"input\", type(input))\n",
        "    # print(\"W\", type(W))\n",
        "    conv2d = tf.nn.relu(tf.nn.conv2d(input, W.detach().numpy(), strides=[1,1,1,1], padding='SAME'))\n",
        "\n",
        "    return tf.reshape(conv2d, [conv2d.shape[1], conv2d.shape[2]])"
      ],
      "metadata": {
        "id": "8ErCjNEnsRx7"
      },
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c88l9LYr9175"
      },
      "source": [
        "class KernelLayer(nn.Module):\n",
        "    def __init__(self, input_shape, n_hid, n_dim, activation, lambda_s=1e-4, lambda_2=1e-4, name=''):\n",
        "        super(KernelLayer, self).__init__()\n",
        "\n",
        "        self.W = nn.Parameter(torch.empty(input_shape[1], n_hid))\n",
        "        nn.init.xavier_uniform_(self.W)\n",
        "\n",
        "        self.u = nn.Parameter(torch.empty(input_shape[1], 1, n_dim).normal_(0, 1e-3))\n",
        "        self.v = nn.Parameter(torch.empty(1, n_hid, n_dim).normal_(0, 1e-3))\n",
        "        self.b = nn.Parameter(torch.zeros(n_hid))\n",
        "\n",
        "        self.activation = activation\n",
        "        self.lambda_s = lambda_s\n",
        "        self.lambda_2 = lambda_2\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        w_hat = local_kernel(self.u, self.v)\n",
        "        # print(\"x.shape\", x.shape)\n",
        "        sparse_reg_term = torch.norm(w_hat, p=2) * self.lambda_s\n",
        "        l2_reg_term = torch.norm(self.W, p=2) * self.lambda_2\n",
        "        \n",
        "        W_eff = self.W * w_hat  # Local kernelised weight matrix\n",
        "        y = torch.matmul(x, W_eff) + self.b\n",
        "        y = self.activation(y)\n",
        "        return y, sparse_reg_term + l2_reg_term"
      ],
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8sQCwrSmKG4"
      },
      "source": [
        "# Network Instantiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOtWj1SCo1RW"
      },
      "source": [
        "## Pre-training\n",
        "One thing to notice here is that I did not implement this part as typical Model. Instead, to ***reconstruct*** the original work, I used a for loop to create such \"model\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7teUrgWagpW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43ca1320-cc58-4d67-fc34-0d7c0b0f9ec3"
      },
      "source": [
        "y = R\n",
        "reg_losses = 0\n",
        "for i in range(n_layers):\n",
        "    # print(y.shape)\n",
        "    layer = KernelLayer(y.shape, n_hid, n_dim, torch.sigmoid, name=str(i))\n",
        "    y, reg_loss = layer(y)\n",
        "    \n",
        "    reg_losses += reg_loss.item()\n",
        "\n",
        "# print(reg_losses)\n",
        "\n",
        "# Compute output and add regularization loss\n",
        "\n",
        "layer_reg = KernelLayer(y.shape, n_u, n_dim, activation=lambda x: x, name='out')\n",
        "pred, reg_loss = layer_reg.forward(y)\n",
        "pred = nn.Parameter(pred)  # wrap pred in a nn.Parameter object\n",
        "reg_losses += reg_loss.item()\n",
        "\n",
        "# Compute L2 loss\n",
        "\n",
        "train_r = torch.tensor(train_r)\n",
        "train_m = torch.tensor(train_m)\n",
        "\n",
        "diff = train_m * (train_r - pred)\n",
        "\n",
        "sqE = torch.nn.functional.mse_loss(diff, torch.zeros_like(diff), reduction='sum')\n",
        "loss_p = sqE + reg_losses\n",
        "print(loss_p)\n",
        "\n",
        "# Use L-BFGS-B optimizer to minimize loss\n",
        "optimizer_p = optim.LBFGS(params=[pred], max_iter=iter_p, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, history_size=10, line_search_fn=None)\n"
      ],
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1111901., grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IEBsNhNo4Cj"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiTXqnN6zLXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2edce6c-8b1d-44a2-8721-9951b2a491c4"
      },
      "source": [
        "y = R\n",
        "reg_losses = None\n",
        "for i in range(n_layers):\n",
        "    layer = KernelLayer(y.shape, n_hid, n_dim, torch.sigmoid, name=str(i))\n",
        "    \n",
        "    y, _ = layer(y)\n",
        "\n",
        "layer_reg = KernelLayer(y.shape, n_u, n_dim, activation=lambda x: x, name='out')\n",
        "y_dash, _ = layer_reg.forward(y)\n",
        "\n",
        "gk = global_kernel(y_dash, gk_size, dot_scale)\n",
        "y_hat = global_conv(train_r, gk)\n",
        "# print(type(y_hat))\n",
        "y_hat = torch.tensor(np.array(y_hat))\n",
        "# print(type(y_hat))\n",
        "# pred = nn.Parameter(pred)  # wrap pred in a nn.Parameter object\n",
        "# reg_losses += reg_loss.item()\n",
        "\n",
        "for j in range(n_layers):\n",
        "    layer_hat = KernelLayer(y_hat.shape , n_hid, n_dim, torch.sigmoid, name=str(i))\n",
        "    y_hat, reg_loss = layer_hat(y_hat)\n",
        "    reg_losses = reg_loss if reg_losses is None else reg_losses + reg_loss\n",
        "\n",
        "\n",
        "layer_tun = KernelLayer(y_hat.shape, n_u, n_dim, activation=lambda x: x, name='out')\n",
        "pred_tun, reg_loss = layer_tun.forward(y_hat)\n",
        "pred_tun = nn.Parameter(pred_tun)  # wrap pred in a nn.Parameter object\n",
        "reg_losses += reg_loss.item()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Compute L2 loss\n",
        "\n",
        "train_r = torch.tensor(train_r)\n",
        "train_m = torch.tensor(train_m)\n",
        "\n",
        "diff_f = train_m * (train_r - pred)\n",
        "\n",
        "sqE = torch.nn.functional.mse_loss(diff_f, torch.zeros_like(diff_f), reduction='sum')\n",
        "loss_f = sqE + reg_losses\n",
        "print(loss_f)\n",
        "\n",
        "\n",
        "optimizer_f = optim.LBFGS(params=[pred_tun], max_iter=iter_f, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, history_size=10, line_search_fn=None)\n",
        "\n",
        "\n"
      ],
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1114689.2500, grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation code"
      ],
      "metadata": {
        "id": "sETwz58aK6y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dcg_k(score_label, k):\n",
        "    dcg, i = 0., 0\n",
        "    for s in score_label:\n",
        "        if i < k:\n",
        "            dcg += (2**s[1]-1) / np.log2(2+i)\n",
        "            i += 1\n",
        "    return dcg"
      ],
      "metadata": {
        "id": "vyReXxgac3KH"
      },
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ndcg_k(y_hat, y, k):\n",
        "    score_label = np.stack([y_hat, y], axis=1).tolist()\n",
        "    score_label = sorted(score_label, key=lambda d:d[0], reverse=True)\n",
        "    score_label_ = sorted(score_label, key=lambda d:d[1], reverse=True)\n",
        "    norm, i = 0., 0\n",
        "    for s in score_label_:\n",
        "        if i < k:\n",
        "            norm += (2**s[1]-1) / np.log2(2+i)\n",
        "            i += 1\n",
        "    dcg = dcg_k(score_label, k)\n",
        "    return dcg / norm"
      ],
      "metadata": {
        "id": "jwsSR-8ZdGWo"
      },
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_ndcg(y_hat, y):\n",
        "    ndcg_sum, num = 0, 0\n",
        "    y_hat, y = y_hat.T, y.T\n",
        "    n_users = y.shape[0]\n",
        "\n",
        "    for i in range(n_users):\n",
        "        y_hat_i = y_hat[i][np.where(y[i])]\n",
        "        y_i = y[i][np.where(y[i])]\n",
        "\n",
        "        if y_i.shape[0] < 2:\n",
        "            continue\n",
        "\n",
        "        ndcg_sum += ndcg_k(y_hat_i, y_i, y_i.shape[0])  # user-wise calculation\n",
        "        num += 1\n",
        "\n",
        "    return ndcg_sum / num"
      ],
      "metadata": {
        "id": "yy9eQS51pbhj"
      },
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXXQjeMxmYEC"
      },
      "source": [
        "# Training and Test Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to the difference between tensorflow and pytorch, here, instead of following the original work, I create a model using nn.Module (also for convenience). "
      ],
      "metadata": {
        "id": "uMt7PXrP8Ji8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ35Zoha-Eue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aad167af-5688-48ee-b463-a69ac9260660"
      },
      "source": [
        "best_rmse_ep, best_mae_ep, best_ndcg_ep = 0, 0, 0\n",
        "best_rmse, best_mae, best_ndcg = float(\"inf\"), float(\"inf\"), 0\n",
        "time_cumulative = 0\n",
        "\n",
        "train_r = torch.tensor(train_r)\n",
        "train_m = torch.tensor(train_m)\n",
        "test_r = torch.tensor(test_r)\n",
        "test_m = torch.tensor(test_m)\n",
        "\n",
        "\n",
        "# Just assume n_layers = 2\n",
        "class KernelModel(nn.Module):\n",
        "    def __init__(self, n_layers, input_shape, n_hid, n_dim, n_u):\n",
        "        super(KernelModel, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for _ in range(n_layers):\n",
        "            layer = KernelLayer(input_shape, n_hid, n_dim, torch.sigmoid)\n",
        "            self.layers.append(layer)\n",
        "            input_shape = (input_shape[0], n_hid)  # Update input_shape for the next layer\n",
        "        self.output_layer = KernelLayer(input_shape, n_u, n_dim, activation=lambda x: x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        reg_losses = 0\n",
        "        for layer in self.layers:\n",
        "            x, reg_loss = layer(x)\n",
        "            reg_losses += reg_loss.item()\n",
        "        pred, reg_loss = self.output_layer(x)\n",
        "        reg_losses += reg_loss.item()\n",
        "        return pred, reg_losses\n",
        "\n",
        "# Initialize the model\n",
        "model_p = KernelModel(n_layers, R.shape, n_hid, n_dim, n_u)\n",
        "\n",
        "# Now you can use model_p in the training loop\n",
        "pre, reg_losses = model_p(train_r)\n",
        "pre = pre.detach().numpy()\n",
        "\n",
        "\n",
        "# Some helpful functions\n",
        "def closure_p():\n",
        "    optimizer_p.zero_grad()\n",
        "    diff = train_m * (train_r - pred)\n",
        "    loss_p = torch.nn.functional.mse_loss(diff, torch.zeros_like(diff), reduction='sum') + reg_losses\n",
        "    loss_p.backward()\n",
        "    return loss_p\n",
        "\n",
        "def closure_f():\n",
        "    optimizer_f.zero_grad()\n",
        "    diff = train_m * (train_r - pred)\n",
        "    loss_p = torch.nn.functional.mse_loss(diff, torch.zeros_like(diff), reduction='sum') + reg_losses\n",
        "    loss_p.backward()\n",
        "    return loss_p\n",
        "\n",
        "def loss_fn(pred, train_r, train_m, clip=True):\n",
        "    diff = train_m * (train_r - pred)\n",
        "    if clip:\n",
        "        diff = torch.clamp(diff, min=1., max=5.)\n",
        "    sqE = torch.nn.functional.mse_loss(diff, torch.zeros_like(diff), reduction='sum')\n",
        "    return sqE\n",
        "\n",
        "# optimizer_p \n",
        "for i in range(epoch_p):\n",
        "  tic = time()\n",
        "  \n",
        "  # Replace the optimizer_p.minimize() with a PyTorch training step\n",
        "\n",
        "  optimizer_p.step(closure_p)\n",
        "\n",
        "  # Replace 'pre' with the output of the PyTorch model\n",
        "  \n",
        "  pre, reg_losses = model_p(train_r)\n",
        "\n",
        "\n",
        "  t = time() - tic\n",
        "  time_cumulative += t\n",
        "\n",
        "  # Calculate error, test_rmse, and train_rmse using PyTorch tensor operations\n",
        "  error = (test_m * (torch.clamp(pre, 1., 5.) - test_r) ** 2).sum() / test_m.sum()\n",
        "  test_rmse = torch.sqrt(error)\n",
        "\n",
        "  error_train = (train_m * (torch.clamp(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()\n",
        "  train_rmse = torch.sqrt(error_train)\n",
        "\n",
        "  print('.-^-._' * 12)\n",
        "  print('PRE-TRAINING')\n",
        "  print('Epoch:', i+1, 'test rmse:', test_rmse, 'train rmse:', train_rmse)\n",
        "  print('Time:', t, 'seconds')\n",
        "  print('Time cumulative:', time_cumulative, 'seconds')\n",
        "  print('.-^-._' * 12)\n",
        "\n",
        "\n",
        "# optimizer_f\n",
        "class KernelModelF(nn.Module):\n",
        "    def __init__(self, n_layers, input_shape, n_hid, n_dim, n_u, gk_size, dot_scale):\n",
        "        super(KernelModelF, self).__init__()\n",
        "        self.layers1 = nn.ModuleList()\n",
        "        self.layers2 = nn.ModuleList()\n",
        "        for _ in range(n_layers):\n",
        "            layer = KernelLayer(input_shape, n_hid, n_dim, torch.sigmoid)\n",
        "            self.layers1.append(layer)\n",
        "            input_shape = (input_shape[0], n_hid)  # Update input_shape for the next layer\n",
        "        for _ in range(n_layers):\n",
        "            layer = KernelLayer(input_shape, n_hid, n_dim, torch.sigmoid)\n",
        "            self.layers2.append(layer)\n",
        "            input_shape = (input_shape[0], n_hid)  # Update input_shape for the next layer\n",
        "        self.output_layer = KernelLayer(input_shape, n_u, n_dim, activation=lambda x: x)\n",
        "        self.gk_size = gk_size\n",
        "        self.dot_scale = dot_scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        reg_losses = 0\n",
        "        for layer in self.layers1:\n",
        "            x, reg_loss = layer(x)\n",
        "            reg_losses += reg_loss.item()\n",
        "        y_dash, reg_loss = self.output_layer(x)\n",
        "        reg_losses += reg_loss.item()\n",
        "\n",
        "        gk = global_kernel(y_dash, self.gk_size, self.dot_scale)\n",
        "        y_hat = global_conv(x.detach().numpy(), gk)\n",
        "        y_hat = torch.tensor(np.array(y_hat))\n",
        "\n",
        "        for layer in self.layers2:\n",
        "            y_hat, reg_loss = layer(y_hat)\n",
        "            reg_losses += reg_loss.item()\n",
        "        pred_tun, reg_loss = self.output_layer(y_hat)\n",
        "        reg_losses += reg_loss.item()\n",
        "\n",
        "        return pred_tun, reg_losses\n",
        "\n",
        "# Initialize the model\n",
        "model_f = KernelModelF(n_layers, R.shape, n_hid, n_dim, n_u, gk_size, dot_scale)\n",
        "\n",
        "test_r = test_r.numpy()\n",
        "test_m = test_m.numpy()\n",
        "# Now you can use model_with_global_kernel in the training loop\n",
        "pred_tun, reg_losses = model_f(train_r)\n",
        "pred_tun = pred_tun.detach().numpy()\n",
        "\n",
        "for i in range(epoch_p):\n",
        "  tic = time()\n",
        "  \n",
        "  # Replace the optimizer_p.minimize() with a PyTorch training step\n",
        "\n",
        "  optimizer_f.step(closure_f)\n",
        "\n",
        "  # Replace 'pre' with the output of the PyTorch model\n",
        "  \n",
        "  pre, reg_losses = model_f(train_r)\n",
        "  pre = pre.detach().numpy()\n",
        "\n",
        "  t = time() - tic\n",
        "  time_cumulative += t\n",
        "\n",
        "  error = (test_m * (np.clip(pre, 1., 5.) - test_r) ** 2).sum() / test_m.sum()  # test error\n",
        "  test_rmse = np.sqrt(error)\n",
        "\n",
        "  error_train = (train_m * (np.clip(pre, 1., 5.) - train_r.numpy()) ** 2).sum() / train_m.sum()  # train error\n",
        "  train_rmse = np.sqrt(error_train)\n",
        "\n",
        "  test_mae = (test_m * np.abs(np.clip(pre, 1., 5.) - test_r)).sum() / test_m.sum()\n",
        "  train_mae = (train_m * np.abs(np.clip(pre, 1., 5.) - train_r.numpy())).sum() / train_m.sum()\n",
        "\n",
        "  test_ndcg = call_ndcg(np.clip(pre, 1., 5.), test_r)\n",
        "  train_ndcg = call_ndcg(np.clip(pre, 1., 5.), train_r)\n",
        "\n",
        "  if test_rmse < best_rmse:\n",
        "      best_rmse = test_rmse\n",
        "      best_rmse_ep = i+1\n",
        "\n",
        "  if test_mae < best_mae:\n",
        "      best_mae = test_mae\n",
        "      best_mae_ep = i+1\n",
        "\n",
        "  if best_ndcg < test_ndcg:\n",
        "      best_ndcg = test_ndcg\n",
        "      best_ndcg_ep = i+1\n",
        "\n",
        "  print('.-^-._' * 12)\n",
        "  print('FINE-TUNING')\n",
        "  print('Epoch:', i+1, 'test rmse:', test_rmse, 'test mae:', test_mae, 'test ndcg:', test_ndcg)\n",
        "  print('Epoch:', i+1, 'train rmse:', train_rmse, 'train mae:', train_mae, 'train ndcg:', train_ndcg)\n",
        "  print('Time:', t, 'seconds')\n",
        "  print('Time cumulative:', time_cumulative, 'seconds')\n",
        "  print('.-^-._' * 12)\n"
      ],
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 1 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.3860139846801758 seconds\n",
            "Time cumulative: 0.3860139846801758 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 2 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.20011019706726074 seconds\n",
            "Time cumulative: 0.5861241817474365 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 3 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.1889195442199707 seconds\n",
            "Time cumulative: 0.7750437259674072 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 4 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.18874716758728027 seconds\n",
            "Time cumulative: 0.9637908935546875 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 5 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.2262716293334961 seconds\n",
            "Time cumulative: 1.1900625228881836 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 6 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.1862020492553711 seconds\n",
            "Time cumulative: 1.3762645721435547 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 7 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.15368914604187012 seconds\n",
            "Time cumulative: 1.5299537181854248 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 8 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.11246967315673828 seconds\n",
            "Time cumulative: 1.642423391342163 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 9 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.11872434616088867 seconds\n",
            "Time cumulative: 1.7611477375030518 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 10 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.11918020248413086 seconds\n",
            "Time cumulative: 1.8803279399871826 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 11 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.11391234397888184 seconds\n",
            "Time cumulative: 1.9942402839660645 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 12 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.1281440258026123 seconds\n",
            "Time cumulative: 2.1223843097686768 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 13 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.12491893768310547 seconds\n",
            "Time cumulative: 2.2473032474517822 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 14 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.11748790740966797 seconds\n",
            "Time cumulative: 2.36479115486145 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 15 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.12333464622497559 seconds\n",
            "Time cumulative: 2.488125801086426 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 16 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.12985658645629883 seconds\n",
            "Time cumulative: 2.6179823875427246 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 17 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.12086296081542969 seconds\n",
            "Time cumulative: 2.7388453483581543 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 18 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.12490367889404297 seconds\n",
            "Time cumulative: 2.8637490272521973 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 19 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.12789106369018555 seconds\n",
            "Time cumulative: 2.991640090942383 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 20 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.12932109832763672 seconds\n",
            "Time cumulative: 3.1209611892700195 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 21 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.11608314514160156 seconds\n",
            "Time cumulative: 3.237044334411621 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 22 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.11729598045349121 seconds\n",
            "Time cumulative: 3.3543403148651123 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 23 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.11571455001831055 seconds\n",
            "Time cumulative: 3.470054864883423 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 24 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.12060999870300293 seconds\n",
            "Time cumulative: 3.590664863586426 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 25 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.11876845359802246 seconds\n",
            "Time cumulative: 3.7094333171844482 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 26 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.14344358444213867 seconds\n",
            "Time cumulative: 3.852876901626587 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 27 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.12067079544067383 seconds\n",
            "Time cumulative: 3.9735476970672607 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 28 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.13165760040283203 seconds\n",
            "Time cumulative: 4.105205297470093 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 29 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.11353564262390137 seconds\n",
            "Time cumulative: 4.218740940093994 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 30 test rmse: tensor(2.7835, grad_fn=<SqrtBackward0>) train rmse: tensor(2.7638, grad_fn=<SqrtBackward0>)\n",
            "Time: 0.12720513343811035 seconds\n",
            "Time cumulative: 4.3459460735321045 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 1 test rmse: 2.7853346 test mae: 2.5352435 test ndcg: 0.8375162096993686\n",
            "Epoch: 1 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.836760332523968\n",
            "Time: 0.28241658210754395 seconds\n",
            "Time cumulative: 4.628362655639648 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 2 test rmse: 2.7853267 test mae: 2.5352361 test ndcg: 0.8368212192692182\n",
            "Epoch: 2 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8367666424359971\n",
            "Time: 0.2946805953979492 seconds\n",
            "Time cumulative: 4.923043251037598 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 3 test rmse: 2.785346 test mae: 2.5352552 test ndcg: 0.8372675025334727\n",
            "Epoch: 3 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8367259695674437\n",
            "Time: 0.2781531810760498 seconds\n",
            "Time cumulative: 5.2011964321136475 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 4 test rmse: 2.7853277 test mae: 2.5352364 test ndcg: 0.8373273658778996\n",
            "Epoch: 4 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8367782161656825\n",
            "Time: 0.27311229705810547 seconds\n",
            "Time cumulative: 5.474308729171753 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 5 test rmse: 2.7853465 test mae: 2.5352552 test ndcg: 0.8373564916366116\n",
            "Epoch: 5 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8370500264311795\n",
            "Time: 0.2750437259674072 seconds\n",
            "Time cumulative: 5.74935245513916 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 6 test rmse: 2.785346 test mae: 2.5352552 test ndcg: 0.8374799480753884\n",
            "Epoch: 6 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8368854992632533\n",
            "Time: 0.2743968963623047 seconds\n",
            "Time cumulative: 6.023749351501465 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 7 test rmse: 2.7853465 test mae: 2.5352552 test ndcg: 0.8370774525795361\n",
            "Epoch: 7 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8369642321621297\n",
            "Time: 0.28289198875427246 seconds\n",
            "Time cumulative: 6.306641340255737 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 8 test rmse: 2.7853453 test mae: 2.5352547 test ndcg: 0.837636795090473\n",
            "Epoch: 8 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8369475043866186\n",
            "Time: 0.35599803924560547 seconds\n",
            "Time cumulative: 6.662639379501343 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 9 test rmse: 2.7853322 test mae: 2.5352416 test ndcg: 0.8374965054457176\n",
            "Epoch: 9 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.836915962285227\n",
            "Time: 0.47681331634521484 seconds\n",
            "Time cumulative: 7.139452695846558 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 10 test rmse: 2.7853465 test mae: 2.5352552 test ndcg: 0.8373564916366116\n",
            "Epoch: 10 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8370500264311795\n",
            "Time: 0.4386153221130371 seconds\n",
            "Time cumulative: 7.578068017959595 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 11 test rmse: 2.7853444 test mae: 2.535254 test ndcg: 0.837000229093421\n",
            "Epoch: 11 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8368547392018365\n",
            "Time: 0.28644347190856934 seconds\n",
            "Time cumulative: 7.864511489868164 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 12 test rmse: 2.7853465 test mae: 2.5352552 test ndcg: 0.8373564916366116\n",
            "Epoch: 12 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8370500264311795\n",
            "Time: 0.2698557376861572 seconds\n",
            "Time cumulative: 8.134367227554321 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 13 test rmse: 2.7853465 test mae: 2.5352552 test ndcg: 0.8372467418769718\n",
            "Epoch: 13 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8370427728641898\n",
            "Time: 0.2767362594604492 seconds\n",
            "Time cumulative: 8.41110348701477 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 14 test rmse: 2.7853465 test mae: 2.5352552 test ndcg: 0.8373564916366116\n",
            "Epoch: 14 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8370500264311795\n",
            "Time: 0.2834601402282715 seconds\n",
            "Time cumulative: 8.694563627243042 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 15 test rmse: 2.7853305 test mae: 2.5352387 test ndcg: 0.8374474089821703\n",
            "Epoch: 15 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8366748731340746\n",
            "Time: 0.275723934173584 seconds\n",
            "Time cumulative: 8.970287561416626 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 16 test rmse: 2.7853465 test mae: 2.5352552 test ndcg: 0.8373564916366116\n",
            "Epoch: 16 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8370500264311795\n",
            "Time: 0.2810556888580322 seconds\n",
            "Time cumulative: 9.251343250274658 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 17 test rmse: 2.7853343 test mae: 2.5352426 test ndcg: 0.8377156036364815\n",
            "Epoch: 17 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8369068836246687\n",
            "Time: 0.31537461280822754 seconds\n",
            "Time cumulative: 9.566717863082886 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 18 test rmse: 2.7853458 test mae: 2.5352547 test ndcg: 0.8373965646461626\n",
            "Epoch: 18 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8368520374702453\n",
            "Time: 0.2792084217071533 seconds\n",
            "Time cumulative: 9.845926284790039 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 19 test rmse: 2.7853465 test mae: 2.5352554 test ndcg: 0.8373610922131817\n",
            "Epoch: 19 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8369326260079718\n",
            "Time: 0.27713513374328613 seconds\n",
            "Time cumulative: 10.123061418533325 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 20 test rmse: 2.785325 test mae: 2.5352347 test ndcg: 0.8370751205025959\n",
            "Epoch: 20 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8368339026312375\n",
            "Time: 0.274979829788208 seconds\n",
            "Time cumulative: 10.398041248321533 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 21 test rmse: 2.7853286 test mae: 2.5352378 test ndcg: 0.8370831303627678\n",
            "Epoch: 21 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8370031514670858\n",
            "Time: 0.28111696243286133 seconds\n",
            "Time cumulative: 10.679158210754395 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 22 test rmse: 2.7853465 test mae: 2.5352552 test ndcg: 0.8373564916366116\n",
            "Epoch: 22 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8370500264311795\n",
            "Time: 0.8749308586120605 seconds\n",
            "Time cumulative: 11.554089069366455 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 23 test rmse: 2.7853196 test mae: 2.535229 test ndcg: 0.8369632269953681\n",
            "Epoch: 23 train rmse: tensor(2.7642) train mae: tensor(2.5279) train ndcg: 0.8369774193665084\n",
            "Time: 0.8680379390716553 seconds\n",
            "Time cumulative: 12.42212700843811 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 24 test rmse: 2.7853284 test mae: 2.5352376 test ndcg: 0.8375101398383167\n",
            "Epoch: 24 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8369374270395922\n",
            "Time: 0.47510600090026855 seconds\n",
            "Time cumulative: 12.897233009338379 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 25 test rmse: 2.7853317 test mae: 2.535241 test ndcg: 0.8372917750978704\n",
            "Epoch: 25 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8367037942989156\n",
            "Time: 0.46483731269836426 seconds\n",
            "Time cumulative: 13.362070322036743 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 26 test rmse: 2.7853465 test mae: 2.5352552 test ndcg: 0.837330516932986\n",
            "Epoch: 26 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8370645313429683\n",
            "Time: 0.28358888626098633 seconds\n",
            "Time cumulative: 13.64565920829773 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 27 test rmse: 2.785346 test mae: 2.5352552 test ndcg: 0.8378764538115717\n",
            "Epoch: 27 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8370062966490001\n",
            "Time: 0.284909725189209 seconds\n",
            "Time cumulative: 13.930568933486938 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 28 test rmse: 2.7853444 test mae: 2.535254 test ndcg: 0.8372968495145906\n",
            "Epoch: 28 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8368533980638618\n",
            "Time: 0.2938070297241211 seconds\n",
            "Time cumulative: 14.22437596321106 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 29 test rmse: 2.7853374 test mae: 2.535247 test ndcg: 0.8374943120146834\n",
            "Epoch: 29 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8368775335534877\n",
            "Time: 0.28088951110839844 seconds\n",
            "Time cumulative: 14.505265474319458 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 30 test rmse: 2.7853465 test mae: 2.5352552 test ndcg: 0.8373564916366116\n",
            "Epoch: 30 train rmse: tensor(2.7643) train mae: tensor(2.5279) train ndcg: 0.8370500264311795\n",
            "Time: 0.27672719955444336 seconds\n",
            "Time cumulative: 14.781992673873901 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTi_PdXJqTjh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d5a78a1-fdb7-4221-bb94-c83581f35ff6"
      },
      "source": [
        "# Final result\n",
        "print('Epoch:', best_rmse_ep, ' best rmse:', best_rmse)\n",
        "print('Epoch:', best_mae_ep, ' best mae:', best_mae)\n",
        "print('Epoch:', best_ndcg_ep, ' best ndcg:', best_ndcg)"
      ],
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 23  best rmse: 2.7853196\n",
            "Epoch: 23  best mae: 2.535229\n",
            "Epoch: 27  best ndcg: 0.8378764538115717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gQkRwBleO7_B"
      }
    }
  ]
}