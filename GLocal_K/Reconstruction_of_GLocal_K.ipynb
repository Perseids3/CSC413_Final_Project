{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MTl_0MrnGSYR",
        "E09YAEjfG1qj",
        "1Qh055wyKHSf",
        "ko0ErrPULWYF",
        "i6aTdzHJMM5J",
        "L4EHMeX0Nvni"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Original Paper: \n",
        "\n",
        "https://arxiv.org/abs/2108.12184\n",
        "\n",
        "Original code and dataset: \n",
        "\n",
        "https://github.com/usydnlp/Glocal_K\n",
        "\n",
        "One part of our project is the reconstruction is \n",
        "the reconstruction of the GLocal_K algorithm with a different approach: we replaced ALL of tensorflow codes with pytorch, which essentially means to redo all the key parts regarding to the model. Other than that, we basically follow the original ideas of the paper and structure of the existing code. "
      ],
      "metadata": {
        "id": "c-Sxlup5gZgk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty3gYQgtnwFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccc80482-7334-4e21-d584-a04b6b86f109"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from zipfile import ZipFile\n",
        "file_name = '/content/data.zip'\n",
        "\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 470,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl2tU6kL8Ot3"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from time import time\n",
        "from scipy.sparse import csc_matrix\n",
        "import numpy as np\n",
        "import h5py"
      ],
      "execution_count": 471,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4A9uU1WloQ2"
      },
      "source": [
        "# Data Loader Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq3KEUaVo1o3"
      },
      "source": [
        "def load_data_100k(path='./', delimiter='\\t'):\n",
        "\n",
        "    train = np.loadtxt(path+'movielens_100k_u1.base', skiprows=0, delimiter=delimiter).astype('int32')\n",
        "    test = np.loadtxt(path+'movielens_100k_u1.test', skiprows=0, delimiter=delimiter).astype('int32')\n",
        "    total = np.concatenate((train, test), axis=0)\n",
        "\n",
        "    n_u = np.unique(total[:,0]).size  # num of users\n",
        "    n_m = np.unique(total[:,1]).size  # num of movies\n",
        "    n_train = train.shape[0]  # num of training ratings\n",
        "    n_test = test.shape[0]  # num of test ratings\n",
        "\n",
        "    train_r = np.zeros((n_m, n_u), dtype='float32')\n",
        "    test_r = np.zeros((n_m, n_u), dtype='float32')\n",
        "\n",
        "    for i in range(n_train):\n",
        "        train_r[train[i,1]-1, train[i,0]-1] = train[i,2]\n",
        "\n",
        "    for i in range(n_test):\n",
        "        test_r[test[i,1]-1, test[i,0]-1] = test[i,2]\n",
        "\n",
        "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
        "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
        "\n",
        "    print('data matrix loaded')\n",
        "    print('num of users: {}'.format(n_u))\n",
        "    print('num of movies: {}'.format(n_m))\n",
        "    print('num of training ratings: {}'.format(n_train))\n",
        "    print('num of test ratings: {}'.format(n_test))\n",
        "\n",
        "    return n_m, n_u, train_r, train_m, test_r, test_m"
      ],
      "execution_count": 472,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3e8Xg3us8g7"
      },
      "source": [
        "def load_data_1m(path='./', delimiter='::', frac=0.1, seed=1234):\n",
        "\n",
        "    tic = time()\n",
        "    print('reading data...')\n",
        "    data = np.loadtxt(path+'movielens_1m_dataset.dat', skiprows=0, delimiter=delimiter).astype('int32')\n",
        "    print('taken', time() - tic, 'seconds')\n",
        "\n",
        "    n_u = np.unique(data[:,0]).size  # num of users\n",
        "    n_m = np.unique(data[:,1]).size  # num of movies\n",
        "    n_r = data.shape[0]  # num of ratings\n",
        "\n",
        "    udict = {}\n",
        "    for i, u in enumerate(np.unique(data[:,0]).tolist()):\n",
        "        udict[u] = i\n",
        "    mdict = {}\n",
        "    for i, m in enumerate(np.unique(data[:,1]).tolist()):\n",
        "        mdict[m] = i\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    idx = np.arange(n_r)\n",
        "    np.random.shuffle(idx)\n",
        "\n",
        "    train_r = np.zeros((n_m, n_u), dtype='float32')\n",
        "    test_r = np.zeros((n_m, n_u), dtype='float32')\n",
        "\n",
        "    for i in range(n_r):\n",
        "        u_id = data[idx[i], 0]\n",
        "        m_id = data[idx[i], 1]\n",
        "        r = data[idx[i], 2]\n",
        "\n",
        "        if i < int(frac * n_r):\n",
        "            test_r[mdict[m_id], udict[u_id]] = r\n",
        "        else:\n",
        "            train_r[mdict[m_id], udict[u_id]] = r\n",
        "\n",
        "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
        "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
        "\n",
        "    print('data matrix loaded')\n",
        "    print('num of users: {}'.format(n_u))\n",
        "    print('num of movies: {}'.format(n_m))\n",
        "    print('num of training ratings: {}'.format(n_r - int(frac * n_r)))\n",
        "    print('num of test ratings: {}'.format(int(frac * n_r)))\n",
        "\n",
        "    return n_m, n_u, train_r, train_m, test_r, test_m"
      ],
      "execution_count": 473,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rMjcbLvhtRs"
      },
      "source": [
        "def load_matlab_file(path_file, name_field):\n",
        "    \n",
        "    db = h5py.File(path_file, 'r')\n",
        "    ds = db[name_field]\n",
        "\n",
        "    try:\n",
        "        if 'ir' in ds.keys():\n",
        "            data = np.asarray(ds['data'])\n",
        "            ir   = np.asarray(ds['ir'])\n",
        "            jc   = np.asarray(ds['jc'])\n",
        "            out  = csc_matrix((data, ir, jc)).astype(np.float32)\n",
        "    except AttributeError:\n",
        "        out = np.asarray(ds).astype(np.float32).T\n",
        "\n",
        "    db.close()\n",
        "\n",
        "    return out"
      ],
      "execution_count": 474,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6pIUrkza2zv"
      },
      "source": [
        "def load_data_monti(path='./'):\n",
        "\n",
        "    M = load_matlab_file(path+'douban_monti_dataset.mat', 'M')\n",
        "    Otraining = load_matlab_file(path+'douban_monti_dataset.mat', 'Otraining') * M\n",
        "    Otest = load_matlab_file(path+'douban_monti_dataset.mat', 'Otest') * M\n",
        "\n",
        "    n_u = M.shape[0]  # num of users\n",
        "    n_m = M.shape[1]  # num of movies\n",
        "    n_train = Otraining[np.where(Otraining)].size  # num of training ratings\n",
        "    n_test = Otest[np.where(Otest)].size  # num of test ratings\n",
        "\n",
        "    train_r = Otraining.T\n",
        "    test_r = Otest.T\n",
        "\n",
        "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
        "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
        "\n",
        "    print('data matrix loaded')\n",
        "    print('num of users: {}'.format(n_u))\n",
        "    print('num of movies: {}'.format(n_m))\n",
        "    print('num of training ratings: {}'.format(n_train))\n",
        "    print('num of test ratings: {}'.format(n_test))\n",
        "\n",
        "    return n_m, n_u, train_r, train_m, test_r, test_m"
      ],
      "execution_count": 475,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_8kEkg9mlIW"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fkA1WpmipzF"
      },
      "source": [
        "# Insert the path of a data directory by yourself (e.g., '/content/.../data')\n",
        "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
        "data_path = '/content/data'\n",
        "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._"
      ],
      "execution_count": 476,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijlu0lXQioYM"
      },
      "source": [
        "# Select a dataset among 'ML-1M', 'ML-100K', and 'Douban'\n",
        "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
        "dataset = 'ML-100K'\n",
        "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._"
      ],
      "execution_count": 477,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJqSSY33mgkw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66b3d222-dc74-4c66-d180-2ac1786d52e5"
      },
      "source": [
        "# Data Load\n",
        "try:\n",
        "    if dataset == 'ML-100K':\n",
        "        path = data_path + '/MovieLens_100K/'\n",
        "        n_m, n_u, train_r, train_m, test_r, test_m = load_data_100k(path=path, delimiter='\\t')\n",
        "\n",
        "    elif dataset == 'ML-1M':\n",
        "        path = data_path + '/MovieLens_1M/'\n",
        "        n_m, n_u, train_r, train_m, test_r, test_m = load_data_1m(path=path, delimiter='::', frac=0.1, seed=1234)\n",
        "\n",
        "    elif dataset == 'Douban':\n",
        "        path = data_path + '/Douban_monti/'\n",
        "        n_m, n_u, train_r, train_m, test_r, test_m = load_data_monti(path=path)\n",
        "\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "except ValueError:\n",
        "    print('Error: Unable to load data')\n",
        "\n",
        "# Dim of data\n",
        "print(\"train_r: \", train_r.shape)\n",
        "print(\"train_m: \", train_m.shape)\n",
        "print(\"test_r: \", test_r.shape)\n",
        "print(\"test_m: \", test_m.shape)\n",
        "print(\"n_m: \", n_m)\n",
        "print(\"n_u: \", n_u)"
      ],
      "execution_count": 478,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data matrix loaded\n",
            "num of users: 943\n",
            "num of movies: 1682\n",
            "num of training ratings: 80000\n",
            "num of test ratings: 20000\n",
            "train_r:  (1682, 943)\n",
            "train_m:  (1682, 943)\n",
            "test_r:  (1682, 943)\n",
            "test_m:  (1682, 943)\n",
            "n_m:  1682\n",
            "n_u:  943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQMtA9yml-gp"
      },
      "source": [
        "# Hyperparameter Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGCdp_FlobOK"
      },
      "source": [
        "# Common hyperparameter settings\n",
        "n_hid = 500\n",
        "n_dim = 5\n",
        "n_layers = 2\n",
        "gk_size = 3"
      ],
      "execution_count": 479,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "344bwGB0cWXp"
      },
      "source": [
        "# Different hyperparameter settings for each dataset\n",
        "if dataset == 'ML-100K':\n",
        "    lambda_2 = 20.  # l2 regularisation\n",
        "    lambda_s = 0.006\n",
        "    # iter_p = 5  # optimisation\n",
        "    # iter_f = 5\n",
        "    epoch_p = 30  # training epoch\n",
        "    epoch_f = 60\n",
        "    dot_scale = 1  # scaled dot product\n",
        "\n",
        "elif dataset == 'ML-1M':\n",
        "    lambda_2 = 70.\n",
        "    lambda_s = 0.018\n",
        "    # iter_p = 50\n",
        "    # iter_f = 10\n",
        "    epoch_p = 20\n",
        "    epoch_f = 30\n",
        "    dot_scale = 0.5\n",
        "\n",
        "elif dataset == 'Douban':\n",
        "    lambda_2 = 10.\n",
        "    lambda_s = 0.022\n",
        "    # iter_p = 5\n",
        "    # iter_f = 5\n",
        "    epoch_p = 20\n",
        "    epoch_f = 60\n",
        "    dot_scale = 2"
      ],
      "execution_count": 480,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b94aimX3nAMI"
      },
      "source": [
        "input_shape = (n_m, n_u)\n",
        "\n",
        "# Create an input tensor\n",
        "R = torch.zeros(input_shape, dtype=torch.float32, requires_grad=True)\n",
        "# print(R.shape)"
      ],
      "execution_count": 481,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sWtU4-pmDDT"
      },
      "source": [
        "# Network Function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Kernel:"
      ],
      "metadata": {
        "id": "voMCI9KwN4tP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def local_kernel(u, v):\n",
        "    dist = torch.norm(u - v, dim=2, p=2)\n",
        "    hat = torch.clamp(1 - dist**2, min=0)\n",
        "    return hat"
      ],
      "metadata": {
        "id": "0x-JEzl7N8bn"
      },
      "execution_count": 482,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Global Kernel"
      ],
      "metadata": {
        "id": "eFEGu8f4sShb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def global_kernel(input, gk_size, dot_scale):\n",
        "    avg_pooling = torch.mean(input, dim=1).unsqueeze(0)\n",
        "    n_kernel = avg_pooling.size(1)\n",
        "\n",
        "    conv_kernel = nn.Parameter(torch.empty(n_kernel, gk_size**2).normal_(0, 0.1))\n",
        "    gk = torch.matmul(avg_pooling, conv_kernel) * dot_scale\n",
        "    gk = gk.view(gk_size, gk_size, 1, 1)\n",
        "\n",
        "    return gk\n",
        "\n",
        "import  tensorflow as tf\n",
        "\n",
        "def global_conv(input, W):\n",
        "\n",
        "    input = torch.reshape(input, [1, 1, input.shape[0], input.shape[1]])\n",
        "    conv2d = torch.nn.functional.relu(torch.nn.functional.conv2d(input, W.detach().numpy(), stride=(1,1), padding=(1,1)))\n",
        "\n",
        "    return torch.reshape(conv2d, [conv2d.shape[2], conv2d.shape[3]])"
      ],
      "metadata": {
        "id": "8ErCjNEnsRx7"
      },
      "execution_count": 483,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c88l9LYr9175"
      },
      "source": [
        "class KernelLayer(nn.Module):\n",
        "    def __init__(self, input_shape, n_hid, n_dim, activation, lambda_s=1e-4, lambda_2=1e-4, name=''):\n",
        "        super(KernelLayer, self).__init__()\n",
        "\n",
        "        self.W = nn.Parameter(torch.empty(input_shape[1], n_hid))\n",
        "        nn.init.xavier_uniform_(self.W)\n",
        "\n",
        "        self.u = nn.Parameter(torch.empty(input_shape[1], 1, n_dim).normal_(0, 1e-3))\n",
        "        self.v = nn.Parameter(torch.empty(1, n_hid, n_dim).normal_(0, 1e-3))\n",
        "        self.b = nn.Parameter(torch.zeros(n_hid))\n",
        "\n",
        "        self.activation = activation\n",
        "        self.lambda_s = lambda_s\n",
        "        self.lambda_2 = lambda_2\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        w_hat = local_kernel(self.u, self.v)\n",
        "        # print(\"x.shape\", x.shape)\n",
        "        sparse_reg_term = torch.norm(w_hat, p=2) * self.lambda_s\n",
        "        l2_reg_term = torch.norm(self.W, p=2) * self.lambda_2\n",
        "        \n",
        "        W_eff = self.W * w_hat  # Local kernelised weight matrix\n",
        "        y = torch.matmul(x, W_eff) + self.b\n",
        "        y = self.activation(y)\n",
        "        return y, sparse_reg_term + l2_reg_term"
      ],
      "execution_count": 484,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8sQCwrSmKG4"
      },
      "source": [
        "# Network Instantiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOtWj1SCo1RW"
      },
      "source": [
        "## Pre-training\n",
        "One thing to notice here is that I did not implement this part (and following parts) as what presented in the original code base.\n",
        "\n",
        "Instead, I used a typical method to implement ***model*** using ***class*** as what we normally do using pytorch and tensorflow 2.x (the original work used 1.x)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7teUrgWagpW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1353f62b-41d9-4012-83d7-3a7cfacac328"
      },
      "source": [
        "class KernelModel(nn.Module):\n",
        "    def __init__(self, n_layers, input_shape, n_hid, n_dim, n_u):\n",
        "        super(KernelModel, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for _ in range(n_layers):\n",
        "            layer = KernelLayer(input_shape, n_hid, n_dim, torch.sigmoid)\n",
        "            self.layers.append(layer)\n",
        "            input_shape = (input_shape[0], n_hid)  # Update input_shape for the next layer\n",
        "        self.output_layer = KernelLayer(input_shape, n_u, n_dim, activation=lambda x: x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        reg_losses = 0\n",
        "        for layer in self.layers:\n",
        "            x, reg_loss = layer(x)\n",
        "            reg_losses += reg_loss.item()\n",
        "        pred, reg_loss = self.output_layer(x)\n",
        "        reg_losses += reg_loss.item()\n",
        "        return pred, reg_losses\n",
        "\n",
        "\n",
        "y = R\n",
        "reg_losses = 0\n",
        "model_p = KernelModel(n_layers, y.shape, n_hid, n_dim, n_u)\n",
        "pred_d, reg_losses = model_p(y)\n",
        "print(reg_losses)\n",
        "\n",
        "# Compute L2 loss (Which is equivalent to mse loss)\n",
        "\n",
        "train_r = torch.tensor(train_r)\n",
        "train_m = torch.tensor(train_m)\n",
        "\n",
        "diff = train_m * (train_r - pred_d)\n",
        "\n",
        "sqE = torch.nn.functional.mse_loss(diff, torch.zeros_like(diff), reduction='sum')\n",
        "loss_p = sqE + reg_losses\n",
        "print(loss_p)\n",
        "\n",
        "# Use Adam optimizer to minimize loss\n",
        "optimizer_p = optim.Adam(model_p.parameters(), lr=0.05)\n"
      ],
      "execution_count": 485,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.19468190521001816\n",
            "tensor(1104852.2500, grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IEBsNhNo4Cj"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiTXqnN6zLXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2754308a-65d4-4ccc-82bd-06426f70dce4"
      },
      "source": [
        "class KernelModelF(nn.Module):\n",
        "    def __init__(self, n_layers, input_shape, n_hid, n_dim, n_u, gk_size, dot_scale):\n",
        "        super(KernelModelF, self).__init__()\n",
        "        self.layers1 = nn.ModuleList()\n",
        "        self.layers2 = nn.ModuleList()\n",
        "        for _ in range(n_layers):\n",
        "            layer = KernelLayer(input_shape, n_hid, n_dim, torch.sigmoid)\n",
        "            self.layers.append(layer)\n",
        "            input_shape = (input_shape[0], n_hid)  # Update input_shape for the next layer\n",
        "        # for _ in range(n_layers):\n",
        "        #     layer = KernelLayer(input_shape, n_hid, n_dim, torch.sigmoid)\n",
        "        #     self.layers2.append(layer)\n",
        "        #     input_shape = (input_shape[0], n_hid)  # Update input_shape for the next layer\n",
        "        self.output_layer = KernelLayer(input_shape, n_u, n_dim, activation=lambda x: x)\n",
        "        self.gk_size = gk_size\n",
        "        self.dot_scale = dot_scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        reg_losses = None\n",
        "        for layer in self.layers:\n",
        "            x, reg_loss = layer(x)\n",
        "\n",
        "        y_dash, _ = self.output_layer(x)\n",
        "\n",
        "\n",
        "        gk = global_kernel(y_dash, self.gk_size, self.dot_scale)\n",
        "\n",
        "        y_hat = global_conv(train_r, gk)\n",
        "        y_hat = torch.tensor(np.array(y_hat))\n",
        "\n",
        "        for layer in self.layers:\n",
        "            y_hat, reg_loss = layer(y_hat)\n",
        "            reg_losses = reg_loss if reg_losses is None else reg_losses + reg_loss\n",
        "        pred_f, reg_loss = self.output_layer(y_hat)\n",
        "        reg_losses += reg_loss.item()\n",
        "\n",
        "        return pred_f, reg_losses\n",
        "\n",
        "\n",
        "\n",
        "y = R\n",
        "reg_losses = None\n",
        "model_f = KernelModel(n_layers, y.shape, n_hid, n_dim, n_u)\n",
        "pred_f, reg_losses = model_f(y)\n",
        "print(reg_losses)\n",
        "\n",
        "# Compute L2 loss\n",
        "\n",
        "train_r = torch.tensor(train_r)\n",
        "train_m = torch.tensor(train_m)\n",
        "\n",
        "diff_f = train_m * (train_r - pred_f)\n",
        "\n",
        "sqE = torch.nn.functional.mse_loss(diff_f, torch.zeros_like(diff_f), reduction='sum')\n",
        "loss_f = sqE + reg_losses\n",
        "print(loss_f)\n",
        "\n",
        "optimizer_f = optim.Adam(model_f.parameters(), lr=0.05)\n",
        "\n"
      ],
      "execution_count": 486,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.19467797875404358\n",
            "tensor(1105990.8750, grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation code"
      ],
      "metadata": {
        "id": "sETwz58aK6y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dcg_k(score_label, k):\n",
        "    dcg, i = 0., 0\n",
        "    for s in score_label:\n",
        "        if i < k:\n",
        "            dcg += (2**s[1]-1) / np.log2(2+i)\n",
        "            i += 1\n",
        "    return dcg"
      ],
      "metadata": {
        "id": "vyReXxgac3KH"
      },
      "execution_count": 487,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ndcg_k(y_hat, y, k):\n",
        "    score_label = np.stack([y_hat, y], axis=1).tolist()\n",
        "    score_label = sorted(score_label, key=lambda d:d[0], reverse=True)\n",
        "    score_label_ = sorted(score_label, key=lambda d:d[1], reverse=True)\n",
        "    norm, i = 0., 0\n",
        "    for s in score_label_:\n",
        "        if i < k:\n",
        "            norm += (2**s[1]-1) / np.log2(2+i)\n",
        "            i += 1\n",
        "    dcg = dcg_k(score_label, k)\n",
        "    return dcg / norm"
      ],
      "metadata": {
        "id": "jwsSR-8ZdGWo"
      },
      "execution_count": 488,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_ndcg(y_hat, y):\n",
        "    ndcg_sum, num = 0, 0\n",
        "    y_hat, y = y_hat.T, y.T\n",
        "    n_users = y.shape[0]\n",
        "\n",
        "    for i in range(n_users):\n",
        "        y_hat_i = y_hat[i][np.where(y[i])]\n",
        "        y_i = y[i][np.where(y[i])]\n",
        "\n",
        "        if y_i.shape[0] < 2:\n",
        "            continue\n",
        "\n",
        "        ndcg_sum += ndcg_k(y_hat_i, y_i, y_i.shape[0])  # user-wise calculation\n",
        "        num += 1\n",
        "\n",
        "    return ndcg_sum / num"
      ],
      "metadata": {
        "id": "yy9eQS51pbhj"
      },
      "execution_count": 489,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXXQjeMxmYEC"
      },
      "source": [
        "# Training and Test Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions:"
      ],
      "metadata": {
        "id": "8x9fo6ZlCBnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def closure_p():\n",
        "    optimizer_p.zero_grad()\n",
        "    pred, reg_losses = model_p(train_r)\n",
        "    diff = train_m * (train_r - pred_p)\n",
        "    loss_p = torch.nn.functional.mse_loss(diff, torch.zeros_like(diff), reduction='sum') + reg_losses\n",
        "    loss_p.backward()\n",
        "    return loss_p\n",
        "\n",
        "def closure_f():\n",
        "    optimizer_f.zero_grad()\n",
        "    diff = train_m * (train_r - pred_f)\n",
        "    loss_p = torch.nn.functional.mse_loss(diff, torch.zeros_like(diff), reduction='sum') + reg_losses\n",
        "    loss_p.backward()\n",
        "    return loss_p\n",
        "\n",
        "def loss_fn(pred, train_r, train_m, clip=True):\n",
        "    diff = train_m * (train_r - pred)\n",
        "    if clip:\n",
        "        diff = torch.clamp(diff, min=1., max=5.)\n",
        "    sqE = torch.nn.functional.mse_loss(diff, torch.zeros_like(diff), reduction='sum')\n",
        "    return sqE"
      ],
      "metadata": {
        "id": "SojfjXcWCFl-"
      },
      "execution_count": 490,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ35Zoha-Eue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dcd661c-c873-4e15-d925-108bea60ce46"
      },
      "source": [
        "# Init some variables\n",
        "best_rmse_ep, best_mae_ep, best_ndcg_ep = 0, 0, 0\n",
        "best_rmse, best_mae, best_ndcg = float(\"inf\"), float(\"inf\"), 0\n",
        "time_cumulative = 0\n",
        "\n",
        "# Initialize the model\n",
        "pred_p, reg_losses = model_p(train_r)\n",
        "\n",
        "# For optimizer_p:\n",
        "for i in range(epoch_p):\n",
        "  tic = time()\n",
        "  optimizer_p.step(closure_p)\n",
        "  pred_p, reg_losses = model_p(train_r)\n",
        "\n",
        "\n",
        "  t = time() - tic\n",
        "  time_cumulative += t\n",
        "\n",
        "  # Calculate error, test_rmse, and train_rmse using PyTorch tensor operations\n",
        "  error = (test_m * (pred_p.detach().numpy() - test_r) ** 2).sum() / test_m.sum()\n",
        "  test_rmse = torch.sqrt(torch.tensor(error))\n",
        "\n",
        "  error_train = (train_m * (pred_p - train_r) ** 2).sum() / train_m.sum()\n",
        "  train_rmse = torch.sqrt(error_train)\n",
        "\n",
        "  print('.-^-._' * 12)\n",
        "  print('PRE-TRAINING')\n",
        "  print('Epoch:', i+1, 'test rmse:', test_rmse.detach().numpy(), 'train rmse:', train_rmse.detach().numpy())\n",
        "  print('Time:', t, 'seconds')\n",
        "  print('Time cumulative:', time_cumulative, 'seconds')\n",
        "  print('.-^-._' * 12)\n",
        "\n",
        "# Now for optimizer_f\n",
        "\n",
        "# Initialize the model\n",
        "pred_f, reg_losses = model_f(train_r)\n",
        "\n",
        "for i in range(epoch_f):\n",
        "  tic = time()\n",
        "  \n",
        "  # Replace the optimizer_p.minimize() with a PyTorch training step\n",
        "\n",
        "  optimizer_f.step(closure_f)\n",
        "\n",
        "  # Replace 'pre' with the output of the PyTorch model\n",
        "  \n",
        "  pred_f, reg_losses = model_f(train_r)\n",
        "  # pred_f = pred_f.detach().numpy()\n",
        "\n",
        "  t = time() - tic\n",
        "  time_cumulative += t\n",
        "  # Following steps are basically copy of the original work\n",
        "  error = (test_m * (pred_f.detach().numpy() - test_r) ** 2).sum() / test_m.sum()  # test error\n",
        "  test_rmse = np.sqrt(torch.tensor(error))\n",
        "\n",
        "  error_train = (train_m * (pred_f - train_r) ** 2).sum() / train_m.sum()  # train error\n",
        "  train_rmse = np.sqrt(error_train.detach().numpy())\n",
        "\n",
        "  test_mae = (test_m * np.abs((pred_f.detach().numpy() - test_r))).sum() / test_m.sum()\n",
        "  \n",
        "  train_mae = (train_m * torch.tensor(np.abs((pred_f - train_r).detach().numpy()))).sum() / train_m.sum()\n",
        "\n",
        "  test_ndcg = call_ndcg(pred_f.detach().numpy(), test_r)\n",
        "  train_ndcg = call_ndcg(pred_f.detach().numpy(), train_r)\n",
        "\n",
        "  if test_rmse < best_rmse:\n",
        "      best_rmse = test_rmse\n",
        "      best_rmse_ep = i+1\n",
        "\n",
        "  if test_mae < best_mae:\n",
        "      best_mae = test_mae\n",
        "      best_mae_ep = i+1\n",
        "\n",
        "  if best_ndcg < test_ndcg:\n",
        "      best_ndcg = test_ndcg\n",
        "      best_ndcg_ep = i+1\n",
        "\n",
        "  print('.-^-._' * 12)\n",
        "  print('FINE-TUNING')\n",
        "  print('Epoch:', i+1, 'test rmse:', test_rmse.detach().numpy(), 'test mae:', test_mae, 'test ndcg:', test_ndcg)\n",
        "  print('Epoch:', i+1, 'train rmse:', train_rmse, 'train mae:', train_mae.detach().numpy(), 'train ndcg:', train_ndcg)\n",
        "  print('Time:', t, 'seconds')\n",
        "  print('Time cumulative:', time_cumulative, 'seconds')\n",
        "  print('.-^-._' * 12)\n"
      ],
      "execution_count": 491,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 1 test rmse: 8.77899 train rmse: 8.921787\n",
            "Time: 0.2527649402618408 seconds\n",
            "Time cumulative: 0.2527649402618408 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 2 test rmse: 1.5280983 train rmse: 1.5699205\n",
            "Time: 0.27286553382873535 seconds\n",
            "Time cumulative: 0.5256304740905762 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 3 test rmse: 4.5751214 train rmse: 4.5672994\n",
            "Time: 0.2767343521118164 seconds\n",
            "Time cumulative: 0.8023648262023926 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 4 test rmse: 5.8203783 train rmse: 5.9227176\n",
            "Time: 0.2787137031555176 seconds\n",
            "Time cumulative: 1.0810785293579102 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 5 test rmse: 4.6317363 train rmse: 4.7862177\n",
            "Time: 0.28586268424987793 seconds\n",
            "Time cumulative: 1.366941213607788 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 6 test rmse: 2.9048324 train rmse: 3.05435\n",
            "Time: 0.28922343254089355 seconds\n",
            "Time cumulative: 1.6561646461486816 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 7 test rmse: 1.7119154 train rmse: 1.8159178\n",
            "Time: 0.30659031867980957 seconds\n",
            "Time cumulative: 1.9627549648284912 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 8 test rmse: 1.2764114 train rmse: 1.278694\n",
            "Time: 0.30064892768859863 seconds\n",
            "Time cumulative: 2.26340389251709 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 9 test rmse: 1.2580217 train rmse: 1.1799259\n",
            "Time: 0.32117533683776855 seconds\n",
            "Time cumulative: 2.5845792293548584 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 10 test rmse: 1.237446 train rmse: 1.1574115\n",
            "Time: 0.3068218231201172 seconds\n",
            "Time cumulative: 2.8914010524749756 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 11 test rmse: 1.1485527 train rmse: 1.0978175\n",
            "Time: 0.32087230682373047 seconds\n",
            "Time cumulative: 3.212273359298706 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 12 test rmse: 1.0942241 train rmse: 1.0573646\n",
            "Time: 0.3016355037689209 seconds\n",
            "Time cumulative: 3.513908863067627 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 13 test rmse: 1.1156787 train rmse: 1.0735862\n",
            "Time: 0.31038808822631836 seconds\n",
            "Time cumulative: 3.8242969512939453 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 14 test rmse: 1.1529763 train rmse: 1.1032323\n",
            "Time: 0.3147561550140381 seconds\n",
            "Time cumulative: 4.139053106307983 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 15 test rmse: 1.1678523 train rmse: 1.1156791\n",
            "Time: 0.30905580520629883 seconds\n",
            "Time cumulative: 4.448108911514282 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 16 test rmse: 1.1579059 train rmse: 1.1086794\n",
            "Time: 0.3150906562805176 seconds\n",
            "Time cumulative: 4.7631995677948 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 17 test rmse: 1.1330462 train rmse: 1.0894136\n",
            "Time: 0.31093668937683105 seconds\n",
            "Time cumulative: 5.074136257171631 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 18 test rmse: 1.1057597 train rmse: 1.0673902\n",
            "Time: 0.3126347064971924 seconds\n",
            "Time cumulative: 5.386770963668823 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 19 test rmse: 1.0878779 train rmse: 1.0514933\n",
            "Time: 0.320554256439209 seconds\n",
            "Time cumulative: 5.707325220108032 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 20 test rmse: 1.0850166 train rmse: 1.0460311\n",
            "Time: 0.31314897537231445 seconds\n",
            "Time cumulative: 6.020474195480347 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 21 test rmse: 1.0928499 train rmse: 1.0481583\n",
            "Time: 0.32003021240234375 seconds\n",
            "Time cumulative: 6.34050440788269 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 22 test rmse: 1.100933 train rmse: 1.0509061\n",
            "Time: 0.31557750701904297 seconds\n",
            "Time cumulative: 6.656081914901733 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 23 test rmse: 1.1017686 train rmse: 1.0496215\n",
            "Time: 0.3097395896911621 seconds\n",
            "Time cumulative: 6.9658215045928955 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 24 test rmse: 1.0956029 train rmse: 1.0453088\n",
            "Time: 0.30416393280029297 seconds\n",
            "Time cumulative: 7.2699854373931885 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 25 test rmse: 1.0879096 train rmse: 1.0419114\n",
            "Time: 0.3163635730743408 seconds\n",
            "Time cumulative: 7.586349010467529 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 26 test rmse: 1.082897 train rmse: 1.041393\n",
            "Time: 0.3022308349609375 seconds\n",
            "Time cumulative: 7.888579845428467 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 27 test rmse: 1.0811727 train rmse: 1.0418999\n",
            "Time: 0.295365571975708 seconds\n",
            "Time cumulative: 8.183945417404175 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 28 test rmse: 1.0791067 train rmse: 1.0403149\n",
            "Time: 0.36043691635131836 seconds\n",
            "Time cumulative: 8.544382333755493 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 29 test rmse: 1.0753962 train rmse: 1.037144\n",
            "Time: 0.4672572612762451 seconds\n",
            "Time cumulative: 9.011639595031738 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 30 test rmse: 1.0711082 train rmse: 1.0339683\n",
            "Time: 0.7423365116119385 seconds\n",
            "Time cumulative: 9.753976106643677 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 1 test rmse: 8.117564 test mae: 8.017438 test ndcg: 0.7973457072220311\n",
            "Epoch: 1 train rmse: 8.177418 train mae: 8.081087 train ndcg: 0.7954334425204969\n",
            "Time: 0.3867502212524414 seconds\n",
            "Time cumulative: 10.140726327896118 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 2 test rmse: 1.5296563 test mae: 1.2049196 test ndcg: 0.8044426234905279\n",
            "Epoch: 2 train rmse: 1.5378414 train mae: 1.2219725 train ndcg: 0.7976251831307144\n",
            "Time: 0.5407469272613525 seconds\n",
            "Time cumulative: 10.68147325515747 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 3 test rmse: 4.257402 test mae: 4.1144814 test ndcg: 0.815203872899194\n",
            "Epoch: 3 train rmse: 4.2523403 train mae: 4.1186533 train ndcg: 0.8074358266327633\n",
            "Time: 0.2087092399597168 seconds\n",
            "Time cumulative: 10.890182495117188 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 4 test rmse: 5.439119 test mae: 5.3126793 test ndcg: 0.8201048871907071\n",
            "Epoch: 4 train rmse: 5.4910216 train mae: 5.3740687 train ndcg: 0.82111507859589\n",
            "Time: 0.2118380069732666 seconds\n",
            "Time cumulative: 11.102020502090454 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 5 test rmse: 4.318583 test mae: 4.142215 test ndcg: 0.8233117022171778\n",
            "Epoch: 5 train rmse: 4.3964553 train mae: 4.231858 train ndcg: 0.8234964909431144\n",
            "Time: 0.2024827003479004 seconds\n",
            "Time cumulative: 11.304503202438354 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 6 test rmse: 2.6795206 test mae: 2.4203942 test ndcg: 0.8248495339659133\n",
            "Epoch: 6 train rmse: 2.7521482 train mae: 2.5033567 train ndcg: 0.820120807886926\n",
            "Time: 0.2538595199584961 seconds\n",
            "Time cumulative: 11.55836272239685 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 7 test rmse: 1.5515078 test mae: 1.2841908 test ndcg: 0.8228208841006086\n",
            "Epoch: 7 train rmse: 1.5857878 train mae: 1.3174407 train ndcg: 0.8209172466680671\n",
            "Time: 0.2327265739440918 seconds\n",
            "Time cumulative: 11.791089296340942 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 8 test rmse: 1.2551166 test mae: 0.99370515 test ndcg: 0.8189392582035737\n",
            "Epoch: 8 train rmse: 1.2155126 train mae: 0.96834767 train ndcg: 0.8170260998987425\n",
            "Time: 0.23343777656555176 seconds\n",
            "Time cumulative: 12.024527072906494 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 9 test rmse: 1.3294035 test mae: 1.0359155 test ndcg: 0.8160606586908534\n",
            "Epoch: 9 train rmse: 1.258857 train mae: 0.98790735 train ndcg: 0.8161709408018216\n",
            "Time: 0.23547720909118652 seconds\n",
            "Time cumulative: 12.26000428199768 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 10 test rmse: 1.2604729 test mae: 0.97484016 test ndcg: 0.8164039036036344\n",
            "Epoch: 10 train rmse: 1.2044771 train mae: 0.9369241 train ndcg: 0.8171482492526122\n",
            "Time: 0.2527921199798584 seconds\n",
            "Time cumulative: 12.512796401977539 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 11 test rmse: 1.1192294 test mae: 0.8717988 test ndcg: 0.8195478777040237\n",
            "Epoch: 11 train rmse: 1.080754 train mae: 0.84715974 train ndcg: 0.8198513488462692\n",
            "Time: 0.23105430603027344 seconds\n",
            "Time cumulative: 12.743850708007812 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 12 test rmse: 1.1094749 test mae: 0.904997 test ndcg: 0.8235201644467386\n",
            "Epoch: 12 train rmse: 1.0748276 train mae: 0.8757434 train ndcg: 0.8248875299456185\n",
            "Time: 0.24036860466003418 seconds\n",
            "Time cumulative: 12.984219312667847 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 13 test rmse: 1.1875856 test mae: 0.9815281 test ndcg: 0.8286715099351235\n",
            "Epoch: 13 train rmse: 1.1480507 train mae: 0.94756925 train ndcg: 0.8300813719153192\n",
            "Time: 0.23286199569702148 seconds\n",
            "Time cumulative: 13.217081308364868 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 14 test rmse: 1.2334982 test mae: 1.0242738 test ndcg: 0.8323285673037609\n",
            "Epoch: 14 train rmse: 1.1921191 train mae: 0.9876151 train ndcg: 0.8329283699541208\n",
            "Time: 0.23270416259765625 seconds\n",
            "Time cumulative: 13.449785470962524 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 15 test rmse: 1.2280132 test mae: 1.0203531 test ndcg: 0.8356168424594822\n",
            "Epoch: 15 train rmse: 1.1882408 train mae: 0.98419374 train ndcg: 0.836646257080871\n",
            "Time: 0.2590210437774658 seconds\n",
            "Time cumulative: 13.70880651473999 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 16 test rmse: 1.1858858 test mae: 0.9838901 test ndcg: 0.8337562493657736\n",
            "Epoch: 16 train rmse: 1.1495503 train mae: 0.95121044 train ndcg: 0.8347325456782871\n",
            "Time: 0.3468356132507324 seconds\n",
            "Time cumulative: 14.055642127990723 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 17 test rmse: 1.1315452 test mae: 0.93088096 test ndcg: 0.8314109178768186\n",
            "Epoch: 17 train rmse: 1.0982485 train mae: 0.9034355 train ndcg: 0.8328809469457175\n",
            "Time: 0.3520975112915039 seconds\n",
            "Time cumulative: 14.407739639282227 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 18 test rmse: 1.0949744 test mae: 0.8850749 test ndcg: 0.8308512037014846\n",
            "Epoch: 18 train rmse: 1.0610683 train mae: 0.8586208 train ndcg: 0.8310534595904405\n",
            "Time: 0.34715819358825684 seconds\n",
            "Time cumulative: 14.754897832870483 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 19 test rmse: 1.095029 test mae: 0.866869 test ndcg: 0.8268990184932286\n",
            "Epoch: 19 train rmse: 1.0551218 train mae: 0.83851165 train ndcg: 0.8282756551365063\n",
            "Time: 0.2531454563140869 seconds\n",
            "Time cumulative: 15.00804328918457 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 20 test rmse: 1.1195235 test mae: 0.87318486 test ndcg: 0.8257654611352727\n",
            "Epoch: 20 train rmse: 1.0709062 train mae: 0.8412586 train ndcg: 0.8257590277894222\n",
            "Time: 0.23874783515930176 seconds\n",
            "Time cumulative: 15.246791124343872 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 21 test rmse: 1.1363957 test mae: 0.8805409 test ndcg: 0.8271748062077625\n",
            "Epoch: 21 train rmse: 1.0823954 train mae: 0.84659725 train ndcg: 0.8270196862652217\n",
            "Time: 0.2429201602935791 seconds\n",
            "Time cumulative: 15.489711284637451 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 22 test rmse: 1.1283073 test mae: 0.87382805 test ndcg: 0.832962757797085\n",
            "Epoch: 22 train rmse: 1.0757277 train mae: 0.841259 train ndcg: 0.8317196277333373\n",
            "Time: 0.23653912544250488 seconds\n",
            "Time cumulative: 15.726250410079956 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 23 test rmse: 1.1055382 test mae: 0.86202663 test ndcg: 0.8413176549127157\n",
            "Epoch: 23 train rmse: 1.0595806 train mae: 0.8330665 train ndcg: 0.8374699155021461\n",
            "Time: 0.2484421730041504 seconds\n",
            "Time cumulative: 15.974692583084106 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 24 test rmse: 1.0874587 test mae: 0.85897726 test ndcg: 0.8433448503341574\n",
            "Epoch: 24 train rmse: 1.048889 train mae: 0.83213526 train ndcg: 0.8409210155897344\n",
            "Time: 0.23254871368408203 seconds\n",
            "Time cumulative: 16.20724129676819 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 25 test rmse: 1.0824779 test mae: 0.86699045 test ndcg: 0.8434795460381826\n",
            "Epoch: 25 train rmse: 1.0488203 train mae: 0.8423364 train ndcg: 0.8405993829908766\n",
            "Time: 0.22815346717834473 seconds\n",
            "Time cumulative: 16.435394763946533 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 26 test rmse: 1.0837338 test mae: 0.8770732 test ndcg: 0.8548487325125674\n",
            "Epoch: 26 train rmse: 1.0520515 train mae: 0.85322773 train ndcg: 0.8499835098049245\n",
            "Time: 0.22168612480163574 seconds\n",
            "Time cumulative: 16.65708088874817 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 27 test rmse: 1.0836664 test mae: 0.8823411 test ndcg: 0.859765381027945\n",
            "Epoch: 27 train rmse: 1.0524191 train mae: 0.85813516 train ndcg: 0.8548523772159685\n",
            "Time: 0.23940110206604004 seconds\n",
            "Time cumulative: 16.89648199081421 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 28 test rmse: 1.0804836 test mae: 0.8798299 test ndcg: 0.8628469496326513\n",
            "Epoch: 28 train rmse: 1.0485189 train mae: 0.85553515 train ndcg: 0.8588337146129286\n",
            "Time: 0.23424220085144043 seconds\n",
            "Time cumulative: 17.13072419166565 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 29 test rmse: 1.0768845 test mae: 0.87264246 test ndcg: 0.8669463424314391\n",
            "Epoch: 29 train rmse: 1.0432508 train mae: 0.84872764 train ndcg: 0.8605324499272397\n",
            "Time: 0.2336881160736084 seconds\n",
            "Time cumulative: 17.364412307739258 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 30 test rmse: 1.0757325 test mae: 0.8648781 test ndcg: 0.867923232416041\n",
            "Epoch: 30 train rmse: 1.0397595 train mae: 0.83974373 train ndcg: 0.8625091408208213\n",
            "Time: 0.230926513671875 seconds\n",
            "Time cumulative: 17.595338821411133 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 31 test rmse: 1.0771141 test mae: 0.86179304 test ndcg: 0.8685494197491619\n",
            "Epoch: 31 train rmse: 1.0385357 train mae: 0.83216757 train ndcg: 0.8636824556813738\n",
            "Time: 0.28500938415527344 seconds\n",
            "Time cumulative: 17.880348205566406 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 32 test rmse: 1.0786417 test mae: 0.8600606 test ndcg: 0.8654663201430152\n",
            "Epoch: 32 train rmse: 1.0378894 train mae: 0.8271532 train ndcg: 0.8610497923314705\n",
            "Time: 0.3499147891998291 seconds\n",
            "Time cumulative: 18.230262994766235 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 33 test rmse: 1.0782291 test mae: 0.85897714 test ndcg: 0.860939483069888\n",
            "Epoch: 33 train rmse: 1.0365354 train mae: 0.8246121 train ndcg: 0.8568970002978278\n",
            "Time: 0.3616504669189453 seconds\n",
            "Time cumulative: 18.59191346168518 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 34 test rmse: 1.0761036 test mae: 0.8575551 test ndcg: 0.8580179212827023\n",
            "Epoch: 34 train rmse: 1.0348825 train mae: 0.8232223 train ndcg: 0.8564523528485432\n",
            "Time: 0.31406235694885254 seconds\n",
            "Time cumulative: 18.905975818634033 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 35 test rmse: 1.0738057 test mae: 0.8570914 test ndcg: 0.8502499127648341\n",
            "Epoch: 35 train rmse: 1.0338646 train mae: 0.82409143 train ndcg: 0.8518939008025609\n",
            "Time: 0.2358253002166748 seconds\n",
            "Time cumulative: 19.141801118850708 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 36 test rmse: 1.0722466 test mae: 0.8584217 test ndcg: 0.8485980812526415\n",
            "Epoch: 36 train rmse: 1.0336559 train mae: 0.82615745 train ndcg: 0.8504502979091437\n",
            "Time: 0.2405102252960205 seconds\n",
            "Time cumulative: 19.38231134414673 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 37 test rmse: 1.0708289 test mae: 0.85921395 test ndcg: 0.8496010125448378\n",
            "Epoch: 37 train rmse: 1.0332699 train mae: 0.8280537 train ndcg: 0.8516810751608512\n",
            "Time: 0.3502027988433838 seconds\n",
            "Time cumulative: 19.732514142990112 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 38 test rmse: 1.0693289 test mae: 0.8599414 test ndcg: 0.8560293027805158\n",
            "Epoch: 38 train rmse: 1.0322257 train mae: 0.82952976 train ndcg: 0.8531891217390064\n",
            "Time: 0.34042930603027344 seconds\n",
            "Time cumulative: 20.072943449020386 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 39 test rmse: 1.0677826 test mae: 0.8589173 test ndcg: 0.8589728446000608\n",
            "Epoch: 39 train rmse: 1.0307453 train mae: 0.829666 train ndcg: 0.8529203304995107\n",
            "Time: 0.338179349899292 seconds\n",
            "Time cumulative: 20.411122798919678 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 40 test rmse: 1.0669402 test mae: 0.8567201 test ndcg: 0.8598781600905264\n",
            "Epoch: 40 train rmse: 1.0297222 train mae: 0.82889104 train ndcg: 0.8561926135646539\n",
            "Time: 0.23732209205627441 seconds\n",
            "Time cumulative: 20.648444890975952 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 41 test rmse: 1.0669987 test mae: 0.85391915 test ndcg: 0.860504718256543\n",
            "Epoch: 41 train rmse: 1.0294389 train mae: 0.82763624 train ndcg: 0.8579468628842678\n",
            "Time: 0.23402929306030273 seconds\n",
            "Time cumulative: 20.882474184036255 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 42 test rmse: 1.0675788 test mae: 0.8516069 test ndcg: 0.8628062905216045\n",
            "Epoch: 42 train rmse: 1.0294781 train mae: 0.82637835 train ndcg: 0.8602937154026395\n",
            "Time: 0.23627328872680664 seconds\n",
            "Time cumulative: 21.11874747276306 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 43 test rmse: 1.0679481 test mae: 0.8499454 test ndcg: 0.8646659757790292\n",
            "Epoch: 43 train rmse: 1.0292937 train mae: 0.82503855 train ndcg: 0.8638150320821338\n",
            "Time: 0.23099732398986816 seconds\n",
            "Time cumulative: 21.34974479675293 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 44 test rmse: 1.0674887 test mae: 0.84867716 test ndcg: 0.8657189234696986\n",
            "Epoch: 44 train rmse: 1.0284741 train mae: 0.8235388 train ndcg: 0.863814964707942\n",
            "Time: 0.230149507522583 seconds\n",
            "Time cumulative: 21.579894304275513 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 45 test rmse: 1.0662842 test mae: 0.84866726 test ndcg: 0.8648113696918889\n",
            "Epoch: 45 train rmse: 1.0272824 train mae: 0.82257736 train ndcg: 0.8636692352306472\n",
            "Time: 0.26683855056762695 seconds\n",
            "Time cumulative: 21.84673285484314 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 46 test rmse: 1.064991 test mae: 0.8497397 test ndcg: 0.8652475486987531\n",
            "Epoch: 46 train rmse: 1.0264351 train mae: 0.82254606 train ndcg: 0.8635738304593871\n",
            "Time: 0.3210451602935791 seconds\n",
            "Time cumulative: 22.16777801513672 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 47 test rmse: 1.0638763 test mae: 0.85093796 test ndcg: 0.8687872146914634\n",
            "Epoch: 47 train rmse: 1.0260406 train mae: 0.8231215 train ndcg: 0.8654105934908147\n",
            "Time: 0.3397707939147949 seconds\n",
            "Time cumulative: 22.507548809051514 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 48 test rmse: 1.0629394 test mae: 0.8514117 test ndcg: 0.8711204292141624\n",
            "Epoch: 48 train rmse: 1.0257064 train mae: 0.8236213 train ndcg: 0.8690389900001648\n",
            "Time: 0.31910276412963867 seconds\n",
            "Time cumulative: 22.826651573181152 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 49 test rmse: 1.0621274 test mae: 0.8507587 test ndcg: 0.8746017247654836\n",
            "Epoch: 49 train rmse: 1.025114 train mae: 0.82258713 train ndcg: 0.8718392721665398\n",
            "Time: 0.23572301864624023 seconds\n",
            "Time cumulative: 23.062374591827393 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 50 test rmse: 1.0612907 test mae: 0.8485368 test ndcg: 0.8763222925720856\n",
            "Epoch: 50 train rmse: 1.024068 train mae: 0.81991327 train ndcg: 0.8733960159330882\n",
            "Time: 0.22516274452209473 seconds\n",
            "Time cumulative: 23.287537336349487 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 51 test rmse: 1.0605057 test mae: 0.8470377 test ndcg: 0.8771366330503888\n",
            "Epoch: 51 train rmse: 1.0230905 train mae: 0.8183162 train ndcg: 0.874841259417504\n",
            "Time: 0.22565484046936035 seconds\n",
            "Time cumulative: 23.513192176818848 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 52 test rmse: 1.0595304 test mae: 0.8463552 test ndcg: 0.8779712279337673\n",
            "Epoch: 52 train rmse: 1.0221559 train mae: 0.81812537 train ndcg: 0.8747676045068112\n",
            "Time: 0.233076810836792 seconds\n",
            "Time cumulative: 23.74626898765564 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 53 test rmse: 1.0579984 test mae: 0.84534717 test ndcg: 0.879019534492622\n",
            "Epoch: 53 train rmse: 1.0207477 train mae: 0.8175369 train ndcg: 0.8751002935571339\n",
            "Time: 0.2339766025543213 seconds\n",
            "Time cumulative: 23.98024559020996 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 54 test rmse: 1.0561303 test mae: 0.8439126 test ndcg: 0.8789660396385579\n",
            "Epoch: 54 train rmse: 1.0189385 train mae: 0.8160934 train ndcg: 0.8759150795831754\n",
            "Time: 0.22251152992248535 seconds\n",
            "Time cumulative: 24.202757120132446 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 55 test rmse: 1.054169 test mae: 0.8425458 test ndcg: 0.8792343744189812\n",
            "Epoch: 55 train rmse: 1.017013 train mae: 0.8147218 train ndcg: 0.8771337636461503\n",
            "Time: 0.22405457496643066 seconds\n",
            "Time cumulative: 24.426811695098877 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 56 test rmse: 1.0522815 test mae: 0.84192413 test ndcg: 0.8795157672704874\n",
            "Epoch: 56 train rmse: 1.0152535 train mae: 0.8141645 train ndcg: 0.8774580961169014\n",
            "Time: 0.2308337688446045 seconds\n",
            "Time cumulative: 24.65764546394348 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 57 test rmse: 1.0505059 test mae: 0.83991903 test ndcg: 0.8807220839970199\n",
            "Epoch: 57 train rmse: 1.0132902 train mae: 0.812241 train ndcg: 0.8783390974594303\n",
            "Time: 0.2172696590423584 seconds\n",
            "Time cumulative: 24.87491512298584 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 58 test rmse: 1.0487056 test mae: 0.83719015 test ndcg: 0.8804994375221188\n",
            "Epoch: 58 train rmse: 1.011201 train mae: 0.80987644 train ndcg: 0.8785642818038585\n",
            "Time: 0.2191150188446045 seconds\n",
            "Time cumulative: 25.094030141830444 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 59 test rmse: 1.0465654 test mae: 0.83357364 test ndcg: 0.8802375727034322\n",
            "Epoch: 59 train rmse: 1.0087494 train mae: 0.8066 train ndcg: 0.877640263801476\n",
            "Time: 0.21515679359436035 seconds\n",
            "Time cumulative: 25.309186935424805 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 60 test rmse: 1.043332 test mae: 0.8311186 test ndcg: 0.8813742018256256\n",
            "Epoch: 60 train rmse: 1.0057728 train mae: 0.8045374 train ndcg: 0.8774822067959401\n",
            "Time: 0.22381591796875 seconds\n",
            "Time cumulative: 25.533002853393555 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTi_PdXJqTjh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9c497f4-826f-494a-f1f0-ac05d744eb35"
      },
      "source": [
        "# Final result\n",
        "print('Epoch:', best_rmse_ep, ' best rmse:', best_rmse.detach().numpy())\n",
        "print('Epoch:', best_mae_ep, ' best mae:', best_mae)\n",
        "print('Epoch:', best_ndcg_ep, ' best ndcg:', best_ndcg)"
      ],
      "execution_count": 492,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 60  best rmse: 1.043332\n",
            "Epoch: 60  best mae: 0.8311186\n",
            "Epoch: 60  best ndcg: 0.8813742018256256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparision with the original work:\n",
        "More detailed writing will be given in the report.\n",
        "To summarize, we have successfully reconstructed the algorithm with an deviation in ~0.1 RMSE."
      ],
      "metadata": {
        "id": "zkQsWjKWHHv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sensitivity Analysis\n",
        "\n",
        "In this section, I will be performing sensitivity analysis by adjusting hyperparameters.\n",
        "\n",
        "Since there is a block for hyperparameters, such adjusting will be quite easy. Thus, here I will then post the results only.\n",
        "\n",
        "\n",
        "Detailed anaylsis will be on the report."
      ],
      "metadata": {
        "id": "MTl_0MrnGSYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base Line (Default Result)\n",
        "Epoch: 60  best rmse: 1.038044\n",
        "\n",
        "Epoch: 49  best mae: 0.82951796\n",
        "\n",
        "Epoch: 60  best ndcg: 0.8719198874988237"
      ],
      "metadata": {
        "id": "2xqlIYawHtaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Epoch: \n"
      ],
      "metadata": {
        "id": "E09YAEjfG1qj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increasize number of epoch by 1.5:\n",
        "\n",
        "Epoch: 90  best rmse: 0.97724164\n",
        "\n",
        "Epoch: 90  best mae: 0.77384365\n",
        "\n",
        "Epoch: 89  best ndcg: 0.8826587666740601\n",
        "\n"
      ],
      "metadata": {
        "id": "Nk5V4rdEJzeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increasize number of epoch by 2:\n",
        "\n",
        "Epoch: 120  best rmse: 0.9544434\n",
        "\n",
        "Epoch: 120  best mae: 0.75268626\n",
        "\n",
        "Epoch: 120  best ndcg: 0.8872194687940944"
      ],
      "metadata": {
        "id": "wuusrjoTJwfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Number of layers:"
      ],
      "metadata": {
        "id": "1Qh055wyKHSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increase the size by 2 (4 layers):\n",
        "\n",
        "Epoch: 60  best rmse: 1.0648588\n",
        "\n",
        "Epoch: 47  best mae: 0.8483205\n",
        "\n",
        "Epoch: 13  best ndcg: 0.85407570236797"
      ],
      "metadata": {
        "id": "5MF5LTiMLFWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hidden Dimension"
      ],
      "metadata": {
        "id": "ko0ErrPULWYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increase the size by 2 (1000):\n",
        "\n",
        "Epoch: 60  best rmse: 1.0306457\n",
        "\n",
        "Epoch: 60  best mae: 0.82486194\n",
        "\n",
        "Epoch: 60  best ndcg: 0.8769577442754222"
      ],
      "metadata": {
        "id": "swvCJxsWLZ5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Different Optimizer:"
      ],
      "metadata": {
        "id": "i6aTdzHJMM5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SGD\n",
        "SGD does not fit this problem. It will generate invalid results."
      ],
      "metadata": {
        "id": "j9cQfgq0MQ36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RMSprop \n",
        "\n",
        "Epoch: 60  best rmse: 1.0599654\n",
        "\n",
        "Epoch: 60  best mae: 0.8478273\n",
        "\n",
        "Epoch: 1  best ndcg: 0.8426037939236917"
      ],
      "metadata": {
        "id": "8EOT9ukjM9e6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning rate (Adam Optimizer):"
      ],
      "metadata": {
        "id": "L4EHMeX0Nvni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increase by 10 times:\n",
        "\n",
        "Epoch: 60  best rmse: 1.0374073\n",
        "\n",
        "Epoch: 60  best mae: 0.8330115\n",
        "\n",
        "Epoch: 60  best ndcg: 0.8731803489067932"
      ],
      "metadata": {
        "id": "_MtBH3h3Nyse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increase by 50 times:\n",
        "\n",
        "Epoch: 60  best rmse: 1.043332\n",
        "\n",
        "Epoch: 60  best mae: 0.8311186\n",
        "\n",
        "Epoch: 60  best ndcg: 0.8813742018256256"
      ],
      "metadata": {
        "id": "7Axafh-mOQJB"
      }
    }
  ]
}