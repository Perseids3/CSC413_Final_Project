{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Readme:\n",
        "\n",
        "This notebook is a demo of GCN (Global Conv Net) for the CSC413 Final Project created by Sanzhe Feng.\n",
        "\n",
        "Original Paper: https://arxiv.org/abs/1703.02719\n",
        "\n"
      ],
      "metadata": {
        "id": "xwl5RSjkfwv4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Goal\n",
        "\n",
        "As mentioned in the report, we cannot fully constructed a GCN due to time and technical restrictions including things like data access. Here we performed a simple demo to just show the doablility of GCN and give the readers a general idea of how can we apply GCN.\n",
        "\n",
        "However, since we are using CIFAR10 dataset, GCN is not a good choice to do classification and thus will not give satisfying results.\n",
        "\n",
        "Again, the idea of the notebook is just to show what does a GCN model look like.\n",
        "\n"
      ],
      "metadata": {
        "id": "x67zM4VGg1kO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "# Define the Global Convolution Network (GCN) architecture\n",
        "class GlobalConvolutionNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Global Convolution Network which can be regarded as Spatial-wise attention.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, k=3, padding=1):\n",
        "        super(GlobalConvolutionNetwork, self).__init__()\n",
        "\n",
        "        self.conv_l1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(k, 1), padding=(padding, 0))\n",
        "        self.conv_l2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=(1, k), padding=(0, padding))\n",
        "\n",
        "        self.conv_r1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(1, k), padding=(0, padding))\n",
        "        self.conv_r2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=(k, 1), padding=(padding, 0))\n",
        "\n",
        "        # Add an adaptive average pooling layer to reduce spatial dimensions to 1x1\n",
        "        self.adaptive_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv_l1(x)\n",
        "        x1 = self.conv_l2(x1)\n",
        "\n",
        "        x2 = self.conv_r1(x)\n",
        "        x2 = self.conv_r2(x2)\n",
        "\n",
        "        out = x1 + x2\n",
        "\n",
        "        # Apply the adaptive average pooling and flatten the tensor\n",
        "        out = self.adaptive_avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# Define a custom dataset and data loader\n",
        "class CustomCIFAR10(Dataset):\n",
        "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
        "        self.cifar10 = CIFAR10(root, train, transform, target_transform, download)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.cifar10[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.cifar10)\n",
        "\n",
        "# Define training and evaluation functions\n",
        "def train(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    return running_loss / len(dataloader), correct / total\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    return running_loss / len(dataloader), correct / total\n"
      ],
      "metadata": {
        "id": "DRurIn4U8mjB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "49208f6a-623d-4047-b266-f2681bd4ddfb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-469ef75fda09>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;31m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_add_docstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_einsum\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopt_einsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_lowrank\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvd_lowrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpca_lowrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m from .overrides import (\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m from .parameter import (\n\u001b[1;32m      3\u001b[0m     \u001b[0mParameter\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mUninitializedParameter\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mUninitializedParameter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mUninitializedBuffer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mUninitializedBuffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mMaxUnpool1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxUnpool2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxUnpool3d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFractionalMaxPool2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFractionalMaxPool3d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLPPool1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLPPool2d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mAdaptiveMaxPool1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdaptiveMaxPool2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdaptiveMaxPool3d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdaptiveAvgPool1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdaptiveAvgPool2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdaptiveAvgPool3d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbatchnorm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchNorm1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchNorm2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchNorm3d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncBatchNorm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mLazyBatchNorm1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyBatchNorm2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyBatchNorm3d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0minstancenorm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInstanceNorm1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInstanceNorm2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInstanceNorm3d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSyncBatchNorm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msync_batch_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlazy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyModuleMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(self, fullname, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_verbose_message\u001b[0;34m(message, verbosity, *args)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Set hyperparameters\n",
        "    num_epochs = 100\n",
        "    batch_size = 100\n",
        "    learning_rate = 0.001\n",
        "\n",
        "    # Prepare data\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ])\n",
        "\n",
        "    train_dataset = CustomCIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "    test_dataset = CustomCIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Set up the device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize the model, criterion, and optimizer\n",
        "    model = GlobalConvolutionNetwork(3, 10).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Train and evaluate the model\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = train(model, train_dataloader, criterion, optimizer, device)\n",
        "        test_loss, test_acc = evaluate(model, test_dataloader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
        "              f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "    # Save the trained model\n",
        "    torch.save(model.state_dict(), \"gcn_model.pth\")\n",
        "    print(\"Model saved successfully.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1AagQ9B9GIs",
        "outputId": "1e0aea64-5e3d-4a5a-ff63-eac99bb57b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 61655844.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Epoch [1/100], Train Loss: 2.1612, Train Acc: 0.1943, Test Loss: 2.0818, Test Acc: 0.2272\n",
            "Epoch [2/100], Train Loss: 2.0798, Train Acc: 0.2222, Test Loss: 2.0621, Test Acc: 0.2321\n",
            "Epoch [3/100], Train Loss: 2.0719, Train Acc: 0.2225, Test Loss: 2.0589, Test Acc: 0.2340\n",
            "Epoch [4/100], Train Loss: 2.0707, Train Acc: 0.2232, Test Loss: 2.0593, Test Acc: 0.2312\n",
            "Epoch [5/100], Train Loss: 2.0700, Train Acc: 0.2245, Test Loss: 2.0565, Test Acc: 0.2336\n",
            "Epoch [6/100], Train Loss: 2.0700, Train Acc: 0.2249, Test Loss: 2.0551, Test Acc: 0.2303\n",
            "Epoch [7/100], Train Loss: 2.0689, Train Acc: 0.2231, Test Loss: 2.0543, Test Acc: 0.2367\n",
            "Epoch [8/100], Train Loss: 2.0692, Train Acc: 0.2256, Test Loss: 2.0609, Test Acc: 0.2340\n",
            "Epoch [9/100], Train Loss: 2.0684, Train Acc: 0.2265, Test Loss: 2.0567, Test Acc: 0.2327\n",
            "Epoch [10/100], Train Loss: 2.0685, Train Acc: 0.2242, Test Loss: 2.0560, Test Acc: 0.2294\n",
            "Epoch [11/100], Train Loss: 2.0671, Train Acc: 0.2264, Test Loss: 2.0546, Test Acc: 0.2309\n",
            "Epoch [12/100], Train Loss: 2.0680, Train Acc: 0.2253, Test Loss: 2.0566, Test Acc: 0.2319\n",
            "Epoch [13/100], Train Loss: 2.0678, Train Acc: 0.2264, Test Loss: 2.0559, Test Acc: 0.2349\n",
            "Epoch [14/100], Train Loss: 2.0666, Train Acc: 0.2278, Test Loss: 2.0532, Test Acc: 0.2322\n",
            "Epoch [15/100], Train Loss: 2.0664, Train Acc: 0.2275, Test Loss: 2.0542, Test Acc: 0.2332\n",
            "Epoch [16/100], Train Loss: 2.0668, Train Acc: 0.2289, Test Loss: 2.0535, Test Acc: 0.2343\n",
            "Epoch [17/100], Train Loss: 2.0665, Train Acc: 0.2282, Test Loss: 2.0536, Test Acc: 0.2373\n",
            "Epoch [18/100], Train Loss: 2.0666, Train Acc: 0.2274, Test Loss: 2.0536, Test Acc: 0.2340\n",
            "Epoch [19/100], Train Loss: 2.0661, Train Acc: 0.2281, Test Loss: 2.0521, Test Acc: 0.2392\n",
            "Epoch [20/100], Train Loss: 2.0654, Train Acc: 0.2282, Test Loss: 2.0535, Test Acc: 0.2342\n",
            "Epoch [21/100], Train Loss: 2.0653, Train Acc: 0.2275, Test Loss: 2.0560, Test Acc: 0.2315\n",
            "Epoch [22/100], Train Loss: 2.0663, Train Acc: 0.2282, Test Loss: 2.0539, Test Acc: 0.2395\n",
            "Epoch [23/100], Train Loss: 2.0659, Train Acc: 0.2287, Test Loss: 2.0502, Test Acc: 0.2346\n",
            "Epoch [24/100], Train Loss: 2.0654, Train Acc: 0.2304, Test Loss: 2.0569, Test Acc: 0.2312\n",
            "Epoch [25/100], Train Loss: 2.0659, Train Acc: 0.2267, Test Loss: 2.0538, Test Acc: 0.2362\n",
            "Epoch [26/100], Train Loss: 2.0661, Train Acc: 0.2286, Test Loss: 2.0534, Test Acc: 0.2364\n",
            "Epoch [27/100], Train Loss: 2.0653, Train Acc: 0.2314, Test Loss: 2.0544, Test Acc: 0.2343\n",
            "Epoch [28/100], Train Loss: 2.0658, Train Acc: 0.2283, Test Loss: 2.0511, Test Acc: 0.2361\n",
            "Epoch [29/100], Train Loss: 2.0655, Train Acc: 0.2285, Test Loss: 2.0519, Test Acc: 0.2378\n",
            "Epoch [30/100], Train Loss: 2.0655, Train Acc: 0.2285, Test Loss: 2.0536, Test Acc: 0.2375\n",
            "Epoch [31/100], Train Loss: 2.0638, Train Acc: 0.2302, Test Loss: 2.0550, Test Acc: 0.2357\n",
            "Epoch [32/100], Train Loss: 2.0659, Train Acc: 0.2298, Test Loss: 2.0533, Test Acc: 0.2355\n",
            "Epoch [33/100], Train Loss: 2.0644, Train Acc: 0.2288, Test Loss: 2.0521, Test Acc: 0.2383\n",
            "Epoch [34/100], Train Loss: 2.0656, Train Acc: 0.2294, Test Loss: 2.0524, Test Acc: 0.2389\n",
            "Epoch [35/100], Train Loss: 2.0644, Train Acc: 0.2296, Test Loss: 2.0521, Test Acc: 0.2331\n",
            "Epoch [36/100], Train Loss: 2.0648, Train Acc: 0.2296, Test Loss: 2.0517, Test Acc: 0.2375\n",
            "Epoch [37/100], Train Loss: 2.0647, Train Acc: 0.2297, Test Loss: 2.0532, Test Acc: 0.2370\n",
            "Epoch [38/100], Train Loss: 2.0646, Train Acc: 0.2297, Test Loss: 2.0540, Test Acc: 0.2324\n",
            "Epoch [39/100], Train Loss: 2.0641, Train Acc: 0.2313, Test Loss: 2.0513, Test Acc: 0.2410\n",
            "Epoch [40/100], Train Loss: 2.0639, Train Acc: 0.2321, Test Loss: 2.0505, Test Acc: 0.2378\n",
            "Epoch [41/100], Train Loss: 2.0642, Train Acc: 0.2305, Test Loss: 2.0512, Test Acc: 0.2403\n",
            "Epoch [42/100], Train Loss: 2.0647, Train Acc: 0.2274, Test Loss: 2.0508, Test Acc: 0.2359\n",
            "Epoch [43/100], Train Loss: 2.0641, Train Acc: 0.2310, Test Loss: 2.0501, Test Acc: 0.2435\n",
            "Epoch [44/100], Train Loss: 2.0632, Train Acc: 0.2307, Test Loss: 2.0525, Test Acc: 0.2363\n",
            "Epoch [45/100], Train Loss: 2.0644, Train Acc: 0.2281, Test Loss: 2.0522, Test Acc: 0.2356\n",
            "Epoch [46/100], Train Loss: 2.0631, Train Acc: 0.2310, Test Loss: 2.0514, Test Acc: 0.2382\n",
            "Epoch [47/100], Train Loss: 2.0645, Train Acc: 0.2311, Test Loss: 2.0502, Test Acc: 0.2373\n",
            "Epoch [48/100], Train Loss: 2.0629, Train Acc: 0.2305, Test Loss: 2.0508, Test Acc: 0.2386\n",
            "Epoch [49/100], Train Loss: 2.0637, Train Acc: 0.2296, Test Loss: 2.0518, Test Acc: 0.2442\n",
            "Epoch [50/100], Train Loss: 2.0629, Train Acc: 0.2347, Test Loss: 2.0503, Test Acc: 0.2376\n",
            "Epoch [51/100], Train Loss: 2.0636, Train Acc: 0.2317, Test Loss: 2.0496, Test Acc: 0.2364\n",
            "Epoch [52/100], Train Loss: 2.0625, Train Acc: 0.2333, Test Loss: 2.0509, Test Acc: 0.2382\n",
            "Epoch [53/100], Train Loss: 2.0620, Train Acc: 0.2340, Test Loss: 2.0509, Test Acc: 0.2436\n",
            "Epoch [54/100], Train Loss: 2.0631, Train Acc: 0.2354, Test Loss: 2.0494, Test Acc: 0.2451\n",
            "Epoch [55/100], Train Loss: 2.0624, Train Acc: 0.2347, Test Loss: 2.0509, Test Acc: 0.2426\n",
            "Epoch [56/100], Train Loss: 2.0610, Train Acc: 0.2351, Test Loss: 2.0493, Test Acc: 0.2426\n",
            "Epoch [57/100], Train Loss: 2.0614, Train Acc: 0.2345, Test Loss: 2.0502, Test Acc: 0.2420\n",
            "Epoch [58/100], Train Loss: 2.0617, Train Acc: 0.2350, Test Loss: 2.0495, Test Acc: 0.2447\n",
            "Epoch [59/100], Train Loss: 2.0625, Train Acc: 0.2328, Test Loss: 2.0492, Test Acc: 0.2383\n",
            "Epoch [60/100], Train Loss: 2.0616, Train Acc: 0.2340, Test Loss: 2.0482, Test Acc: 0.2382\n",
            "Epoch [61/100], Train Loss: 2.0607, Train Acc: 0.2360, Test Loss: 2.0492, Test Acc: 0.2445\n",
            "Epoch [62/100], Train Loss: 2.0616, Train Acc: 0.2343, Test Loss: 2.0496, Test Acc: 0.2441\n",
            "Epoch [63/100], Train Loss: 2.0605, Train Acc: 0.2359, Test Loss: 2.0461, Test Acc: 0.2474\n",
            "Epoch [64/100], Train Loss: 2.0616, Train Acc: 0.2361, Test Loss: 2.0466, Test Acc: 0.2389\n",
            "Epoch [65/100], Train Loss: 2.0605, Train Acc: 0.2363, Test Loss: 2.0495, Test Acc: 0.2366\n",
            "Epoch [66/100], Train Loss: 2.0598, Train Acc: 0.2372, Test Loss: 2.0475, Test Acc: 0.2460\n",
            "Epoch [67/100], Train Loss: 2.0599, Train Acc: 0.2370, Test Loss: 2.0464, Test Acc: 0.2468\n",
            "Epoch [68/100], Train Loss: 2.0601, Train Acc: 0.2337, Test Loss: 2.0493, Test Acc: 0.2396\n",
            "Epoch [69/100], Train Loss: 2.0601, Train Acc: 0.2388, Test Loss: 2.0443, Test Acc: 0.2434\n",
            "Epoch [70/100], Train Loss: 2.0598, Train Acc: 0.2350, Test Loss: 2.0452, Test Acc: 0.2491\n",
            "Epoch [71/100], Train Loss: 2.0594, Train Acc: 0.2364, Test Loss: 2.0479, Test Acc: 0.2479\n",
            "Epoch [72/100], Train Loss: 2.0591, Train Acc: 0.2377, Test Loss: 2.0453, Test Acc: 0.2506\n",
            "Epoch [73/100], Train Loss: 2.0591, Train Acc: 0.2381, Test Loss: 2.0467, Test Acc: 0.2425\n",
            "Epoch [74/100], Train Loss: 2.0600, Train Acc: 0.2377, Test Loss: 2.0460, Test Acc: 0.2498\n",
            "Epoch [75/100], Train Loss: 2.0590, Train Acc: 0.2387, Test Loss: 2.0468, Test Acc: 0.2435\n",
            "Epoch [76/100], Train Loss: 2.0577, Train Acc: 0.2383, Test Loss: 2.0450, Test Acc: 0.2459\n",
            "Epoch [77/100], Train Loss: 2.0589, Train Acc: 0.2368, Test Loss: 2.0480, Test Acc: 0.2443\n",
            "Epoch [78/100], Train Loss: 2.0579, Train Acc: 0.2372, Test Loss: 2.0473, Test Acc: 0.2465\n",
            "Epoch [79/100], Train Loss: 2.0575, Train Acc: 0.2384, Test Loss: 2.0442, Test Acc: 0.2445\n",
            "Epoch [80/100], Train Loss: 2.0579, Train Acc: 0.2381, Test Loss: 2.0442, Test Acc: 0.2470\n",
            "Epoch [81/100], Train Loss: 2.0583, Train Acc: 0.2389, Test Loss: 2.0444, Test Acc: 0.2443\n",
            "Epoch [82/100], Train Loss: 2.0576, Train Acc: 0.2379, Test Loss: 2.0423, Test Acc: 0.2452\n",
            "Epoch [83/100], Train Loss: 2.0559, Train Acc: 0.2422, Test Loss: 2.0416, Test Acc: 0.2523\n",
            "Epoch [84/100], Train Loss: 2.0570, Train Acc: 0.2386, Test Loss: 2.0440, Test Acc: 0.2482\n",
            "Epoch [85/100], Train Loss: 2.0572, Train Acc: 0.2393, Test Loss: 2.0460, Test Acc: 0.2444\n",
            "Epoch [86/100], Train Loss: 2.0563, Train Acc: 0.2433, Test Loss: 2.0438, Test Acc: 0.2466\n",
            "Epoch [87/100], Train Loss: 2.0555, Train Acc: 0.2430, Test Loss: 2.0460, Test Acc: 0.2442\n",
            "Epoch [88/100], Train Loss: 2.0552, Train Acc: 0.2422, Test Loss: 2.0442, Test Acc: 0.2478\n",
            "Epoch [89/100], Train Loss: 2.0560, Train Acc: 0.2405, Test Loss: 2.0436, Test Acc: 0.2490\n",
            "Epoch [90/100], Train Loss: 2.0572, Train Acc: 0.2445, Test Loss: 2.0421, Test Acc: 0.2514\n",
            "Epoch [91/100], Train Loss: 2.0553, Train Acc: 0.2401, Test Loss: 2.0422, Test Acc: 0.2533\n",
            "Epoch [92/100], Train Loss: 2.0557, Train Acc: 0.2412, Test Loss: 2.0422, Test Acc: 0.2571\n",
            "Epoch [93/100], Train Loss: 2.0557, Train Acc: 0.2432, Test Loss: 2.0445, Test Acc: 0.2564\n",
            "Epoch [94/100], Train Loss: 2.0560, Train Acc: 0.2407, Test Loss: 2.0419, Test Acc: 0.2480\n",
            "Epoch [95/100], Train Loss: 2.0547, Train Acc: 0.2447, Test Loss: 2.0435, Test Acc: 0.2489\n",
            "Epoch [96/100], Train Loss: 2.0546, Train Acc: 0.2451, Test Loss: 2.0399, Test Acc: 0.2529\n",
            "Epoch [97/100], Train Loss: 2.0548, Train Acc: 0.2422, Test Loss: 2.0411, Test Acc: 0.2570\n",
            "Epoch [98/100], Train Loss: 2.0560, Train Acc: 0.2423, Test Loss: 2.0410, Test Acc: 0.2483\n",
            "Epoch [99/100], Train Loss: 2.0552, Train Acc: 0.2437, Test Loss: 2.0410, Test Acc: 0.2514\n",
            "Epoch [100/100], Train Loss: 2.0547, Train Acc: 0.2424, Test Loss: 2.0403, Test Acc: 0.2525\n",
            "Model saved successfully.\n"
          ]
        }
      ]
    }
  ]
}